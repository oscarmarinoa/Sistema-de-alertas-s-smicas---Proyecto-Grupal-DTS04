{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Índice`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Importación de librerías`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "#Para rellenar vacíos\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#ETL:\n",
    "\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#para normalizar fechas\n",
    "import datetime\n",
    "#para obtener fechas\n",
    "import time\n",
    "#hacer los calendarios de iteración\n",
    "from dateutil.rrule import rrule, DAILY , MONTHLY\n",
    "\n",
    "#Conexión con postgresql:\n",
    "\n",
    "#Para crear tablas con claves primarias y foraneas\n",
    "import psycopg2\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Ver para japón: (borrar si no se usa)\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "#limpiar la consola y acceder a directorios\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Directorio de backup`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La creación del directorio c:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms/backup falló\n",
      "La creación del directorio c:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms/backup/usa falló\n",
      "La creación del directorio c:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms/backup/japon falló\n",
      "La creación del directorio c:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms/backup/chile falló\n"
     ]
    }
   ],
   "source": [
    "#Se rutea los directorios y en caso de no existir se crean\n",
    "# Se define el nombre de la carpeta o directorio a crear\n",
    "path=pathlib.Path().absolute()\n",
    "directorio = f\"{path}/backup\"\n",
    "try:\n",
    "    os.mkdir(directorio)\n",
    "except OSError:\n",
    "    print(\"La creación del directorio %s falló\" % directorio)\n",
    "else:\n",
    "    print(\"Se ha creado el directorio: %s \" % directorio)\n",
    "# Hago sub carpetas con el nombre de los paises\n",
    "directorios=['usa','japon','chile']\n",
    "for direc in directorios:\n",
    "    ruta_backs = f\"{directorio}/{direc}\"\n",
    "    try:\n",
    "        os.mkdir(ruta_backs)\n",
    "    except OSError:\n",
    "        print(\"La creación del directorio %s falló\" % ruta_backs)\n",
    "    else:\n",
    "        print(\"Se ha creado el directorio: %s \" % ruta_backs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Carga (Load - Potgresql)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexión a postgres mediante alquemy\n",
    "cone = create_engine('postgresql://sismosu:123@localhost:5432/sismosdb', pool_size=50, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "\n",
    "tabla_paises = 'DROP TABLE IF EXISTS PAIS CASCADE; CREATE TABLE PAIS (idpais SERIAL PRIMARY KEY NOT NULL ,pais text);'\n",
    "tabla_eeuu = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (idsismo SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami text, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_chile = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER, foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami text, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_japon = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami text, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami text, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tsunamis = 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),altura_oleaje float8, place text, time timestamp, url text, mag float8, lng float8, lat float8, deepth float8);'\n",
    "volcanes = 'DROP TABLE IF EXISTS VOLCANES CASCADE; CREATE TABLE VOLCANES (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),nombre text, tipo text, elevacion float8, place text, ultima_erupcion text, lat float8, lng float8, url text);'\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='sismosu',\n",
    "    password='123',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(tabla_paises)\n",
    "cur.execute(tabla_eeuu)\n",
    "cur.execute(tabla_chile)\n",
    "cur.execute(tabla_japon)\n",
    "cur.execute(tabla_hechos)\n",
    "cur.execute(tsunamis)\n",
    "cur.execute(volcanes)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Definimos tabla de paises`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paises(pais):\n",
    "    '''\n",
    "    Función:Generar claves id pais\n",
    "    Entrada: pais a dar/crear clave\n",
    "    Salida:clave idpais\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    global dic\n",
    "    if pais in dic:\n",
    "        #df.to_sql(name='pais',con=cone, if_exists='replace', index=False)\n",
    "        return(dic[pais])\n",
    "    else:\n",
    "        dic[pais]=len(dic)+1\n",
    "        dic_pais={'pais':[pais]}\n",
    "        df=pd.DataFrame(dic_pais)\n",
    "        df.to_sql(name='pais',con=cone, if_exists='append', index=False)\n",
    "        return(dic[pais])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Definimos fecha de inicio y fin de la carga de datos`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fechas():\n",
    "    ''' \n",
    "    Función:Definimos las fechas de inicio y fin\n",
    "    Entrada:\n",
    "    Salida: Fechas a cargar en db\n",
    "    '''\n",
    "    #Fecha inicio\n",
    "    a = datetime.date(2022, 1, 1)\n",
    "    #Fecha fin\n",
    "    b = datetime.date.today()\n",
    "    return a,b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`ETL`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla (df):\n",
    "    '''\n",
    "    Función: limpieza de cadenas de string\n",
    "    Entrada: Data Frame a normalizar\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace = True) \n",
    "    #Acomodamos el indice\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)                  \n",
    "        #sacamos los acentos\n",
    "        df[c]=df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c== 'place':\n",
    " \n",
    "            lista_palabras=[' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_palabras:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c]=df[c].apply(lambda x: 'sin dato' if type(x)== str and x=='' else x)\n",
    "            #sacamos los que no tengan el pais que buscamos\n",
    "            df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')] \n",
    "            #los eliminamos de place\n",
    "            lista_simbolos=['japan','chile']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)\n",
    "\n",
    "        #detectamos NaN\n",
    "        df[c]=df[c].apply(lambda x: None if type(x)== str and x=='' else x)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Japón`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Japón instantanea`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def japon_instantanea():\n",
    "    #creo dicc vacio\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    '''\n",
    "    límite\n",
    "    entero ($ int32 )\n",
    "    número de devoluciones (1-100, el valor predeterminado es 10)\n",
    "    '''\n",
    "    limit=100\n",
    "    '''\n",
    "    compensar\n",
    "    entero ($ int32 )\n",
    "    Número de registros para omitir \n",
    "    (0 o más, el valor predeterminado es 0). \n",
    "    Por ejemplo, limit=100&offset=200 devuelve 100 registros desde el 201.\n",
    "    '''\n",
    "    offset=0\n",
    "    '''\n",
    "    desde_fecha\n",
    "    Fecha especificada o posterior (formato aaaaMMdd)\n",
    "    '''\n",
    "    a,b=fechas()\n",
    "    fecha_start=int(b.strftime('%Y%m%d'))\n",
    "\n",
    "    fecha_end= int(a.strftime('%Y%m%d'))\n",
    "\n",
    "    #creo un bucle para recorrer los diferentes offset sin superar la fecha mínima registrada   \n",
    "    while offset<=2100 and fecha_start>=fecha_end :\n",
    "        #Obtengo url personalizada para la fecha\n",
    "        urlP2p = f\"https://api.p2pquake.net/v1/human-readable?limit=100&offset={offset}&since_date={fecha_start}\"\n",
    "        #hago la request en la api\n",
    "        responseP2p = requests.get(urlP2p)\n",
    "        #cambio el formato a tipo json\n",
    "        jsonDataP2p = responseP2p.json()\n",
    "        #itero por la cantidad de datos que pedí\n",
    "        for i in range(limit):\n",
    "            #intento poblar el diccionario\n",
    "            try:\n",
    "                #primero me fijo que pueda acceder a esa fila\n",
    "                try:a=jsonDataP2p[i]\n",
    "                except:pass\n",
    "                #Pueblo el diccionario y en caso de nulos completo el valor con None\n",
    "                #Magnitud:            \n",
    "                try:dict['mag'].append(float(a['earthquake']['hypocenter']['magnitude']))\n",
    "                except:dict['mag'].append(None)\n",
    "                #Lugar\n",
    "                try:dict['place'].append(a['points'][0]['addr'])\n",
    "                except:dict['place'].append(None)\n",
    "                #Fecha\n",
    "                try:dict['time'].append(datetime.datetime.strptime(a['time'], '%Y/%m/%d %H:%M:%S.%f'))            \n",
    "                #try:dict['time'].append(a['time'])\n",
    "                except:dict['time'].append(None)\n",
    "                #Url\n",
    "                #modifico el índice con el que se puede acceder a cada valor\n",
    "                indice=offset+i\n",
    "                try:dict['url'].append(f\"https://api.p2pquake.net/v1/human-readable?limit=1&offset={indice}&since_date={fecha_start}\")\n",
    "                except:dict['url'].append(None)\n",
    "                #indico si hay tsunami registrado            \n",
    "                try:dict['tsunami'].append(a['earthquake'][\"domesticTsunami\"])\n",
    "                except:dict['tsunami'].append(None)\n",
    "                #Dejo la columna title como complementaria a place          \n",
    "                try:dict['title'].append(a['earthquake']['hypocenter']['name'])\n",
    "                except:dict['title'].append(None)\n",
    "                #Cologo longitud\n",
    "                try:dict['lng'].append(float(a['earthquake']['hypocenter']['longitude'][1:]))\n",
    "                except:dict['lng'].append(None)\n",
    "                #coloco latitud\n",
    "                try:dict['lat'].append(float(a['earthquake']['hypocenter']['latitude'][1:]))\n",
    "                except:dict['lat'].append(None)\n",
    "                #coloco profundidad\n",
    "                try:dict['deepth'].append(float(a['earthquake']['hypocenter']['depth'][:-2]))\n",
    "                except:dict['deepth'].append(None)\n",
    "            except: \n",
    "                pass\n",
    "        #incremento el offset\n",
    "        offset+=100\n",
    "    df=pd.DataFrame(dict)\n",
    "    #borro duplicados\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    #filtro filas con vacíos\n",
    "    filtros=[None,'']\n",
    "    for i in filtros:\n",
    "        #Creo máscaras \n",
    "        m1=df['mag'].values !=i\n",
    "        m2=df['title'].values !=i\n",
    "        m3=df['lng'].values !=i\n",
    "        m4=df['lat'].values !=i\n",
    "        m5=df['deepth'].values !=i\n",
    "        #Aplico las máscaras\n",
    "        df=df[m1 & m2 & m3 & m4 & m5]\n",
    "    df.insert(loc = 0, column = 'idpais', value =paises('japon'))\n",
    "    df['peligro']=1\n",
    "    df.to_csv(f'{directorio}/japon/japon_instantanea.csv')\n",
    "    #Paso la tabla a Postgres\n",
    "    df.to_sql(name='japon',con=cone, if_exists='append', index=False)#<-- DESACTIVAR/ACTIVAR NO/SI SE QUIERE PASAR A POSTGRES\n",
    "    #Paso a tabla de Postgres única de hechos\n",
    "    df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gise\\AppData\\Local\\Temp\\ipykernel_5736\\1059274433.py:84: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  m1=df['mag'].values !=i\n",
      "C:\\Users\\Gise\\AppData\\Local\\Temp\\ipykernel_5736\\1059274433.py:86: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  m3=df['lng'].values !=i\n",
      "C:\\Users\\Gise\\AppData\\Local\\Temp\\ipykernel_5736\\1059274433.py:87: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  m4=df['lat'].values !=i\n",
      "C:\\Users\\Gise\\AppData\\Local\\Temp\\ipykernel_5736\\1059274433.py:88: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  m5=df['deepth'].values !=i\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>deepth</th>\n",
       "      <th>peligro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>鹿児島十島村</td>\n",
       "      <td>2022-11-24 03:14:39.017</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>トカラ列島近海</td>\n",
       "      <td>129.2</td>\n",
       "      <td>29.3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>蘭越町</td>\n",
       "      <td>2022-11-24 02:28:32.636</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>後志地方西部</td>\n",
       "      <td>140.6</td>\n",
       "      <td>42.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-11-24 02:27:51.935</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>後志地方西部</td>\n",
       "      <td>140.6</td>\n",
       "      <td>42.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>日立市</td>\n",
       "      <td>2022-11-24 01:57:18.207</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>八丈島東方沖</td>\n",
       "      <td>141.5</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>八戸市</td>\n",
       "      <td>2022-11-24 01:32:42.698</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>北海道西方沖</td>\n",
       "      <td>140.2</td>\n",
       "      <td>44.2</td>\n",
       "      <td>260.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>珠洲市</td>\n",
       "      <td>2022-11-15 06:24:14.540</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>137.2</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>珠洲市</td>\n",
       "      <td>2022-11-14 22:31:07.509</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>137.3</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-11-14 22:30:14.905</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>137.3</td>\n",
       "      <td>37.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>双葉町</td>\n",
       "      <td>2022-11-14 17:14:27.801</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>三重県南東沖</td>\n",
       "      <td>137.5</td>\n",
       "      <td>33.8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-11-14 17:12:25.913</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>三重県南東沖</td>\n",
       "      <td>137.5</td>\n",
       "      <td>33.8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1452 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      idpais  mag   place                    time  \\\n",
       "0          1  2.0  鹿児島十島村 2022-11-24 03:14:39.017   \n",
       "1          1  3.3     蘭越町 2022-11-24 02:28:32.636   \n",
       "2          1  3.3    None 2022-11-24 02:27:51.935   \n",
       "4          1  5.2     日立市 2022-11-24 01:57:18.207   \n",
       "7          1  4.5     八戸市 2022-11-24 01:32:42.698   \n",
       "...      ...  ...     ...                     ...   \n",
       "2189       1  2.9     珠洲市 2022-11-15 06:24:14.540   \n",
       "2190       1  4.2     珠洲市 2022-11-14 22:31:07.509   \n",
       "2191       1  4.2    None 2022-11-14 22:30:14.905   \n",
       "2195       1  6.1     双葉町 2022-11-14 17:14:27.801   \n",
       "2196       1  6.1    None 2022-11-14 17:12:25.913   \n",
       "\n",
       "                                                    url tsunami    title  \\\n",
       "0     https://api.p2pquake.net/v1/human-readable?lim...    None  トカラ列島近海   \n",
       "1     https://api.p2pquake.net/v1/human-readable?lim...    None   後志地方西部   \n",
       "2     https://api.p2pquake.net/v1/human-readable?lim...    None   後志地方西部   \n",
       "4     https://api.p2pquake.net/v1/human-readable?lim...    None   八丈島東方沖   \n",
       "7     https://api.p2pquake.net/v1/human-readable?lim...    None   北海道西方沖   \n",
       "...                                                 ...     ...      ...   \n",
       "2189  https://api.p2pquake.net/v1/human-readable?lim...    None  石川県能登地方   \n",
       "2190  https://api.p2pquake.net/v1/human-readable?lim...    None  石川県能登地方   \n",
       "2191  https://api.p2pquake.net/v1/human-readable?lim...    None  石川県能登地方   \n",
       "2195  https://api.p2pquake.net/v1/human-readable?lim...    None   三重県南東沖   \n",
       "2196  https://api.p2pquake.net/v1/human-readable?lim...    None   三重県南東沖   \n",
       "\n",
       "        lng   lat  deepth  peligro  \n",
       "0     129.2  29.3    10.0        1  \n",
       "1     140.6  42.9     NaN        1  \n",
       "2     140.6  42.9     NaN        1  \n",
       "4     141.5  33.8    40.0        1  \n",
       "7     140.2  44.2   260.0        1  \n",
       "...     ...   ...     ...      ...  \n",
       "2189  137.2  37.5    10.0        1  \n",
       "2190  137.3  37.5    10.0        1  \n",
       "2191  137.3  37.5    10.0        1  \n",
       "2195  137.5  33.8   350.0        1  \n",
       "2196  137.5  33.8   350.0        1  \n",
       "\n",
       "[1452 rows x 11 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=japon_instantanea()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Japón histórico`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Web scraping\n",
    "#Permiso de la web\n",
    "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'\n",
    "#desactivamos los request\n",
    "disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(date):\n",
    "    '''\n",
    "    Función:Generar la url custom para la fecha y los datos para esa fecha\n",
    "    Entrada: Toma como ingesta la fecha en formato %YYYY%mm%dd, ej 20221102\n",
    "    Salida: Devuelve la url para esa fecha y las filas de la tabla obtenida para esa fecha\n",
    "    '''\n",
    "    #url\n",
    "    url=f'https://www.hinet.bosai.go.jp/AQUA/aqua_catalogue.php?y={date.year}&m={date.month}&LANG=en'\n",
    "    #hacemos la request\n",
    "    page = requests.get(url)\n",
    "    #leemos el html\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "    #Pasamos a filas\n",
    "    rows1 = soup.find(\"table\", attrs={\"class\":\"base\"}).find_all(\"tr\")\n",
    "    return url,rows1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historicos_japon(url,rows1):\n",
    "    '''\n",
    "    Función:\n",
    "    Entrada: Toma como ingesta la url para una fecha y las filas\n",
    "    Salida: Devuelve un df con todos los sismos de japón para todo el mes de esa fecha\n",
    "    '''\n",
    "    #creo dicc vacio\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    for index in range(5,len(rows1)-1):\n",
    "        dict['mag'].append(float(rows1[index].find_all(\"td\")[5].get_text()))\n",
    "        dict['place'].append(rows1[index].find_all(\"td\")[1].get_text())\n",
    "        dict['time'].append(datetime.datetime.strptime(rows1[index].find_all(\"td\")[0].get_text(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        dict['url'].append(url)\n",
    "        dict['tsunami'].append(None)\n",
    "        dict['title'].append(None)\n",
    "\n",
    "        dict['lng'].append(float(rows1[index].find_all(\"td\")[3].get_text()[:-1]))\n",
    "        dict['lat'].append(float(rows1[index].find_all(\"td\")[2].get_text()[:-1]))\n",
    "        dict['deepth'].append(float(rows1[index].find_all(\"td\")[4].get_text().split('km')[0]))\n",
    "    for elemento in dict:\n",
    "        len(dict[elemento])\n",
    "    df_japon=pd.DataFrame(dict)\n",
    "    return df_japon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_historica_japon():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    a,b=fechas()\n",
    "    for dt in rrule(MONTHLY, dtstart=a, until=b):\n",
    "        #print (dt.strftime(\"%Y%m%d\"))\n",
    "        url,rows1=get_url(dt)\n",
    "        df=historicos_japon(url,rows1)\n",
    "        df.insert(loc = 0, column = 'idpais', value =paises('japon'))\n",
    "        df['peligro']=1\n",
    "        df.to_csv(f'{directorio}/japon/japon_historico_{dt.strftime(\"%Y-%m-%d\")}.csv')\n",
    "        #Paso la tabla a Postgres\n",
    "        df.to_sql(name='japon',con=cone, if_exists='append', index=False)#<-- DESACTIVAR/ACTIVAR NO/SI SE QUIERE PASAR A POSTGRES\n",
    "        #Paso a tabla de Postgres única de hechos\n",
    "        df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "        #display(df)\n",
    "    return ('La carga se hizo con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La carga se hizo con éxito'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carga_historica_japon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Japón en el segundo`***\n",
    "Acá: Sacar última información de japón, para alimentar db y crear alertas - TIRA LA MÁX EN EL MOMENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>int</th>\n",
       "      <th>peligro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japon</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Adjacent Sea of Tokara Islands</td>\n",
       "      <td>2022-11-24 03:14:00+09:00</td>\n",
       "      <td>https://www.jma.go.jp/bosai/quake/data/2022112...</td>\n",
       "      <td>This earthquake poses no tsunami risk.\\n</td>\n",
       "      <td>震源・震度情報</td>\n",
       "      <td>129.33</td>\n",
       "      <td>29.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idpais  mag                           place                      time  \\\n",
       "0  japon  2.0  Adjacent Sea of Tokara Islands 2022-11-24 03:14:00+09:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.jma.go.jp/bosai/quake/data/2022112...   \n",
       "\n",
       "                                    tsunami    title     lng    lat  int  \\\n",
       "0  This earthquake poses no tsunami risk.\\n  震源・震度情報  129.33  29.22  1.0   \n",
       "\n",
       "   peligro  \n",
       "0        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#creo dicc vacio\n",
    "dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'int':[]}\n",
    "#pido los últimos sismos\n",
    "urlJMAList = \"https://www.jma.go.jp/bosai/quake/data/list.json\"\n",
    "#hago una lista de los últimos sismos\n",
    "responseJMAList = requests.get(urlJMAList)\n",
    "#la convierto a formato json\n",
    "jsonDataJMAList = responseJMAList.json()\n",
    "\n",
    "i=0\n",
    "#recorro la lista y solicito la info a la api\n",
    "#for elemento in range(len(jsonDataJMAList)):\n",
    "jsonDataJMALastDatetime = jsonDataJMAList[0][\"rdt\"]\n",
    "\n",
    "jsonDataJMALastDatetime = datetime.datetime.strptime(jsonDataJMALastDatetime, '%Y-%m-%dT%H:%M:%S+09:00')\n",
    "\n",
    "urlJMAEarthquakeData = \"https://www.jma.go.jp/bosai/quake/data/\" + jsonDataJMAList[i][\"json\"]\n",
    "\n",
    "responseJMAEarthquakeData = requests.get(urlJMAEarthquakeData)\n",
    "\n",
    "jsonDataJMAEarthquakeData = responseJMAEarthquakeData.json()\n",
    "#'mag'\n",
    "dict['mag'].append(float(jsonDataJMAEarthquakeData['Body']['Earthquake']['Magnitude']))\n",
    "#'place\n",
    "dict['place'].append(jsonDataJMAEarthquakeData['Body']['Earthquake']['Hypocenter']['Area']['enName'] )\n",
    "#time\n",
    "dict['time'].append(datetime.datetime.strptime(jsonDataJMAEarthquakeData['Head']['ReportDateTime'],'%Y-%m-%dT%H:%M:%S%z'))\n",
    "#url\n",
    "dict['url'].append(urlJMAEarthquakeData)\n",
    "#tsunami\n",
    "dict['tsunami'].append(jsonDataJMAEarthquakeData['Body']['Comments']['ForecastComment']['enText'])\n",
    "\n",
    "#title\n",
    "dict['title'].append(jsonDataJMAEarthquakeData['Head']['Title'])\n",
    "\n",
    "#lat, long\n",
    "dict['lat'].append(float(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lat']))\n",
    "dict['lng'].append(float(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lon']))\n",
    "\n",
    "#intensidad:\n",
    "dict['int'].append(float(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['MaxInt']))\n",
    "i+=1\n",
    "df=pd.DataFrame(dict)\n",
    "df.insert(loc = 0, column = 'idpais', value ='japon')\n",
    "df['peligro']=1\n",
    "#Paso la tabla a Postgres-- tiene intensidad no magnitud por lo que quizá no sirva\n",
    "#df.to_sql(name='japon',con=cone, if_exists='append', index=False)#<-- DESACTIVAR/ACTIVAR NO/SI SE QUIERE PASAR A POSTGRES\n",
    "#Paso a tabla de Postgres única de hechos\n",
    "#df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Chile`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Chile histórica`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_chile(dt):\n",
    "    year=dt.strftime(\"%Y\")\n",
    "    month=dt.strftime(\"%m\")\n",
    "    day=dt.strftime('%Y%m%d')\n",
    "\n",
    "    url=f\"https://www.sismologia.cl/sismicidad/catalogo/{year}/{month}/{day}.html\"\n",
    "    urlx = requests.get (url)\n",
    "    soup = BeautifulSoup(urlx.content,\"html.parser\")\n",
    "    rows = soup.find(\"table\", attrs={\"class\":\"sismologia detalle\"}).find_all(\"tr\")\n",
    "    return  url, rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos_chile(url,rows):\n",
    "    #creo dicc vacio\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    for index,i in enumerate(rows):\n",
    "        if index==0:\n",
    "            pass\n",
    "        else:\n",
    "            dict['mag'].append(float(rows[index].find_all(\"td\")[-1].get_text()[:-3]))\n",
    "            dict['place'].append(rows[index].find_all(\"td\")[0].get_text()[19:])\n",
    "            dict['time'].append(datetime.datetime.strptime(rows[index].find_all(\"td\")[1].get_text(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "            dict['url'].append(url)\n",
    "            dict['tsunami'].append(None)\n",
    "            dict['title'].append(None)\n",
    "\n",
    "            dict['lng'].append(float(rows[index].find_all(\"td\")[2].get_text()[:7]))\n",
    "            dict['lat'].append(float(rows[index].find_all(\"td\")[2].get_text()[-7:]))\n",
    "            dict['deepth'].append(float(rows[index].find_all(\"td\")[3].get_text().split()[0]))\n",
    "    for elemento in dict:\n",
    "        len(dict[elemento])\n",
    "    df_chile=pd.DataFrame(dict)\n",
    "    return df_chile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabla_chile(dt):\n",
    "    url, rows=get_url_chile(dt)\n",
    "    df_chile=datos_chile(url,rows)\n",
    "    return df_chile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_historica_chile():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    a,b =fechas()\n",
    "    for dt in rrule(DAILY, dtstart=a, until=b):\n",
    "        url, rows=get_url_chile(dt)    \n",
    "        df=datos_chile(url,rows)\n",
    "        \n",
    "        df.insert(loc = 0, column = 'idpais', value =paises('chile'))\n",
    "        df['peligro']=1\n",
    "        df.to_csv(f'{directorio}/chile/chile_historico_{dt.strftime(\"%Y-%m-%d\")}.csv')\n",
    "        #Paso la tabla a Postgres\n",
    "        df.to_sql(name='chile',con=cone, if_exists='append', index=False)#<-- DESACTIVAR/ACTIVAR NO/SI SE QUIERE PASAR A POSTGRES\n",
    "        #Paso a tabla de Postgres única de hechos\n",
    "        df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "        #display(df)\n",
    "    return ('La carga se hizo con éxito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La carga se hizo con éxito'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carga_historica_chile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Carga de datos de USGS`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos (url):\n",
    "    '''\n",
    "    Función: pasar de los datos de url de USGS\n",
    "    Entrada: Toma de parámetro una url y devuelve el df que utilizaremos\n",
    "    Salida: DataFrame con los datos de los sismos en la fecha\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    for i in range(len(resp['features'])):\n",
    "        \n",
    "        a=resp['features'][i]\n",
    "        #agrego al diccionario magnitud, tiempo, si produjo tsunami \n",
    "        lista_propierties=['mag','time']\n",
    "        for elemento in lista_propierties:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(np.nan) \n",
    "            else:\n",
    "                dict[elemento].append(float(a['properties'][elemento]))\n",
    "                \n",
    "        #agrego lugar, url con información a ampliar y el título     \n",
    "        lista_propierties2=['place','url','title','tsunami']\n",
    "        for elemento in lista_propierties2:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(None)\n",
    "            else:\n",
    "                dict[elemento].append(a['properties'][elemento])\n",
    "\n",
    "        #Agrego al diccionario latitud, longitud y profundidad\n",
    "        list_geometry=['lng','lat','deepth']\n",
    "        for indice, elemento in enumerate(list_geometry):\n",
    "            if a['geometry']['coordinates'][indice] is None:\n",
    "                dict[elemento].append(np.nan)\n",
    "            else:\n",
    "                dict[elemento].append(float(a['geometry']['coordinates'][indice]))\n",
    "\n",
    "    #Devuelvo el diccionario hecho df\n",
    "    return (pd.DataFrame(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_paises():\n",
    "    \n",
    "    '''\n",
    "    Función: Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en tres tablas distintas\n",
    "    Sirve para respaldo\n",
    "    Ingresa: año, mes y fin_día\n",
    "    Salida: devuelve una tabla con las fechas que tuvieron errores durante la carga en una tabla de auditoría\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises_list = ['usa', 'japon', 'chile']\n",
    "    #Creo listas vacías donde colocaré los errores que sucedan durante la carga, para utilizar a modo de auditoría\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "    \n",
    "    a,b=fechas()\n",
    "    print(a,b)\n",
    "    #Guardo una lista de las fechas a trabajar\n",
    "    lista_fechas=[]\n",
    "    \n",
    "    for dt in rrule(MONTHLY, dtstart=a, until=b):\n",
    "        lista_fechas.append(dt.strftime('%Y-%m-%d'))\n",
    "    lista_fechas.append(b.strftime('%Y-%m-%d'))\n",
    "\n",
    "    for pais in paises_list:\n",
    "        for indice_fecha in range(len(lista_fechas)-1):\n",
    "\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={lista_fechas[indice_fecha]}&endtime={lista_fechas[indice_fecha+1]}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={lista_fechas[indice_fecha]}&endtime={lista_fechas[indice_fecha+1]}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={lista_fechas[indice_fecha]}&endtime={lista_fechas[indice_fecha+1]}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url)\n",
    "\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "\n",
    "                #pasamos a formato fecha\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df.insert(loc = 0, column = 'idpais', value =paises(pais))\n",
    "                \n",
    "                #normalizamos cadenas de string\n",
    "                #df=limpieza_general_tabla(df)\n",
    "                df['peligro']=1\n",
    "                #imprimo backup de cada carga de tabla:\n",
    "                df.to_csv(f'{directorio}/{pais}/{pais}desde{lista_fechas[indice_fecha]}hasta{lista_fechas[indice_fecha+1]}.csv')\n",
    "                #Paso la tabla a Postgres\n",
    "                df.to_sql(name=pais,con=cone, if_exists='append', index=False)#<-- DESACTIVAR/ACTIVAR NO/SI SE QUIERE PASAR A POSTGRES\n",
    "                #Paso a tabla de Postgres única de hechos\n",
    "                df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "                #print('La carga se ha hecho con exito!en la fecha:', lista_fechas[indice_fecha],'hasta',lista_fechas[indice_fecha+1])\n",
    "            except:\n",
    "                #Agrego a la lista de errores\n",
    "                print('Error en la carga en la fecha:', lista_fechas[indice_fecha],'hasta',lista_fechas[indice_fecha+1],'del país:',pais)\n",
    "        print(f'La carga se ha hecho con exito!en pais: {pais}')\n",
    "    #Df de auditoría de errores durante la carga\n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "\n",
    "    return(errores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 2022-11-23\n",
      "La carga se ha hecho con exito!en pais: usa\n",
      "La carga se ha hecho con exito!en pais: japon\n",
      "La carga se ha hecho con exito!en pais: chile\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_inicio</th>\n",
       "      <th>fecha_final</th>\n",
       "      <th>pais</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [fecha_inicio, fecha_final, pais]\n",
       "Index: []"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cargar_paises()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=limpieza_general_tabla(df)\n",
    "df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')]\n",
    "\n",
    "ERROR: raise AttributeError(\"Can only use .str accessor with string values!\")\n",
    "\n",
    "Error en la carga en la fecha: 2020-6-01 hasta 2020-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2019-7-01 hasta 2019-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-7-01 hasta 2018-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-6-01 hasta 2018-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2010-4-01 hasta 2010-4-30 del país: usa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`NOAAA`***\n",
    "Documentation:\n",
    "* https://www.ngdc.noaa.gov/hazel/view/swagger#/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Tsunamis`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsunami_Noaa():\n",
    "    ''' \n",
    "    Función:\n",
    "    Entrada:\n",
    "    Salida:    \n",
    "    '''\n",
    "    countries = ['chile', 'japan', 'usa']\n",
    "    for country in countries:\n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?country={country.upper()}&maxYear=2022&minYear=2000'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Creamos la columna con la fecha del tsunami\n",
    "        df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']],format='%Y/%m/%d %H:%M:%S')\n",
    "        df['time'] =df['time'].apply(lambda x: x.strftime('%Y/%m/%d %H:%M:%S'))\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'deepth', 'latitude':'lat', 'longitude':'lng',\n",
    "                   'locationName':'place', 'country':'pais', 'maxWaterHeight':'altura_oleaje'}, inplace=True)\n",
    "\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        if country=='japan':\n",
    "            country='japon'\n",
    "        df.insert(loc = 0, column = 'idpais', value =paises(country))\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "        df['altura_oleaje']  = df.altura_oleaje.astype('float64')\n",
    "        df.mag=df.mag.astype('float64')\n",
    "        df.lat=df.lng.astype('float64')\n",
    "        df.lat=df.lat.astype('float64')\n",
    "        df.deepth=df.deepth.astype('float64')\n",
    "\n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','altura_oleaje','place', 'time', 'url', 'mag', 'lng',\n",
    "            'lat', 'deepth' ]]\n",
    "    \n",
    "       \n",
    "        display(df)\n",
    "\n",
    "        #Info para terminar de cargar la tabla\n",
    "        # #Paso la tabla a Potgres\n",
    "        df.to_sql(name=tsunamis,con=cone, if_exists='append', index=False)\n",
    "        print(f'La carga se ha hecho con exito!{country}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>altura_oleaje</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>mag</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>deepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.14</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2010/03/11 14:39:43</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-71.891</td>\n",
       "      <td>-71.891</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.16</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2020/12/27 21:39:14</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-74.990</td>\n",
       "      <td>-74.990</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.63</td>\n",
       "      <td>northern chile</td>\n",
       "      <td>2014/04/01 23:46:26</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-70.769</td>\n",
       "      <td>-70.769</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>northern chile</td>\n",
       "      <td>2007/11/14 15:40:50</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-69.890</td>\n",
       "      <td>-69.890</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>13.60</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2015/09/16 22:55:26</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-71.674</td>\n",
       "      <td>-71.674</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0.55</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2015/11/11 01:54:00</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-72.120</td>\n",
       "      <td>-72.120</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>50.00</td>\n",
       "      <td>southern chile</td>\n",
       "      <td>2007/04/21 17:53:46</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.2</td>\n",
       "      <td>-72.606</td>\n",
       "      <td>-72.606</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>29.00</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2010/02/27 06:34:11</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>-72.898</td>\n",
       "      <td>-72.898</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.74</td>\n",
       "      <td>northern chile</td>\n",
       "      <td>2014/04/03 02:43:11</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-70.493</td>\n",
       "      <td>-70.493</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>southern chile</td>\n",
       "      <td>2016/12/25 14:22:00</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>7.6</td>\n",
       "      <td>-74.391</td>\n",
       "      <td>-74.391</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>0.07</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2019/06/14 00:19:12</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.4</td>\n",
       "      <td>-72.082</td>\n",
       "      <td>-72.082</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>0.26</td>\n",
       "      <td>northern chile</td>\n",
       "      <td>2014/03/16 21:16:00</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-70.814</td>\n",
       "      <td>-70.814</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2011/02/11 20:05:30</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>-73.125</td>\n",
       "      <td>-73.125</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "      <td>easter island region</td>\n",
       "      <td>2014/10/09 02:14:44</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-110.811</td>\n",
       "      <td>-110.811</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>0.16</td>\n",
       "      <td>central chile</td>\n",
       "      <td>2017/04/24 21:39:22</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "      <td>6.9</td>\n",
       "      <td>-72.062</td>\n",
       "      <td>-72.062</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    idpais  altura_oleaje                 place                 time  \\\n",
       "0        2           0.14         central chile  2010/03/11 14:39:43   \n",
       "1        2           0.16         central chile  2020/12/27 21:39:14   \n",
       "2        2           4.63        northern chile  2014/04/01 23:46:26   \n",
       "3        2           0.28        northern chile  2007/11/14 15:40:50   \n",
       "4        2          13.60         central chile  2015/09/16 22:55:26   \n",
       "5        2           0.55         central chile  2015/11/11 01:54:00   \n",
       "6        2          50.00        southern chile  2007/04/21 17:53:46   \n",
       "7        2          29.00         central chile  2010/02/27 06:34:11   \n",
       "8        2           0.74        northern chile  2014/04/03 02:43:11   \n",
       "9        2           0.44        southern chile  2016/12/25 14:22:00   \n",
       "10       2           0.07         central chile  2019/06/14 00:19:12   \n",
       "11       2           0.26        northern chile  2014/03/16 21:16:00   \n",
       "12       2           0.15         central chile  2011/02/11 20:05:30   \n",
       "13       2           0.47  easter island region  2014/10/09 02:14:44   \n",
       "14       2           0.16         central chile  2017/04/24 21:39:22   \n",
       "\n",
       "                                                  url  mag      lng      lat  \\\n",
       "0   https://www.ngdc.noaa.gov/hazel/hazard-service...  6.9  -71.891  -71.891   \n",
       "1   https://www.ngdc.noaa.gov/hazel/hazard-service...  6.7  -74.990  -74.990   \n",
       "2   https://www.ngdc.noaa.gov/hazel/hazard-service...  8.2  -70.769  -70.769   \n",
       "3   https://www.ngdc.noaa.gov/hazel/hazard-service...  7.7  -69.890  -69.890   \n",
       "4   https://www.ngdc.noaa.gov/hazel/hazard-service...  8.3  -71.674  -71.674   \n",
       "5   https://www.ngdc.noaa.gov/hazel/hazard-service...  6.9  -72.120  -72.120   \n",
       "6   https://www.ngdc.noaa.gov/hazel/hazard-service...  6.2  -72.606  -72.606   \n",
       "7   https://www.ngdc.noaa.gov/hazel/hazard-service...  8.8  -72.898  -72.898   \n",
       "8   https://www.ngdc.noaa.gov/hazel/hazard-service...  7.7  -70.493  -70.493   \n",
       "9   https://www.ngdc.noaa.gov/hazel/hazard-service...  7.6  -74.391  -74.391   \n",
       "10  https://www.ngdc.noaa.gov/hazel/hazard-service...  6.4  -72.082  -72.082   \n",
       "11  https://www.ngdc.noaa.gov/hazel/hazard-service...  6.7  -70.814  -70.814   \n",
       "12  https://www.ngdc.noaa.gov/hazel/hazard-service...  6.8  -73.125  -73.125   \n",
       "13  https://www.ngdc.noaa.gov/hazel/hazard-service...  7.0 -110.811 -110.811   \n",
       "14  https://www.ngdc.noaa.gov/hazel/hazard-service...  6.9  -72.062  -72.062   \n",
       "\n",
       "    deepth  \n",
       "0     11.0  \n",
       "1     10.0  \n",
       "2     25.0  \n",
       "3     40.0  \n",
       "4     22.0  \n",
       "5     33.0  \n",
       "6      NaN  \n",
       "7     23.0  \n",
       "8     22.0  \n",
       "9     30.0  \n",
       "10    11.0  \n",
       "11    21.0  \n",
       "12    28.0  \n",
       "13    16.0  \n",
       "14    28.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IdentifierError",
     "evalue": "Identifier 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),altura_oleaje float8, place text, time timestamp, url text, mag float8, lng float8, lat float8, deepth float8);' exceeds maximum length of 63 characters",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIdentifierError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms\\scripts_datasets.ipynb Celda 45\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df1\u001b[39m=\u001b[39mtsunami_Noaa()\n",
      "\u001b[1;32mc:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms\\scripts_datasets.ipynb Celda 45\u001b[0m in \u001b[0;36mtsunami_Noaa\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     display(df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m#Info para terminar de cargar la tabla\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# #Paso la tabla a Potgres\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     df\u001b[39m.\u001b[39;49mto_sql(name\u001b[39m=\u001b[39;49mtsunamis,con\u001b[39m=\u001b[39;49mcone, if_exists\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mappend\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLa carga se ha hecho con exito!\u001b[39m\u001b[39m{\u001b[39;00mcountry\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/scripts_datasets.ipynb#Y100sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2963\u001b[0m, in \u001b[0;36mNDFrame.to_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2806\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2807\u001b[0m \u001b[39mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   2808\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2959\u001b[0m \u001b[39m[(1,), (None,), (2,)]\u001b[39;00m\n\u001b[0;32m   2960\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa:E501\u001b[39;00m\n\u001b[0;32m   2961\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m sql\n\u001b[1;32m-> 2963\u001b[0m \u001b[39mreturn\u001b[39;00m sql\u001b[39m.\u001b[39;49mto_sql(\n\u001b[0;32m   2964\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2965\u001b[0m     name,\n\u001b[0;32m   2966\u001b[0m     con,\n\u001b[0;32m   2967\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   2968\u001b[0m     if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[0;32m   2969\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   2970\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   2971\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   2972\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2973\u001b[0m     method\u001b[39m=\u001b[39;49mmethod,\n\u001b[0;32m   2974\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:697\u001b[0m, in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(frame, DataFrame):\n\u001b[0;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument should be either a Series or a DataFrame\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m     )\n\u001b[1;32m--> 697\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39mto_sql(\n\u001b[0;32m    698\u001b[0m     frame,\n\u001b[0;32m    699\u001b[0m     name,\n\u001b[0;32m    700\u001b[0m     if_exists\u001b[39m=\u001b[39mif_exists,\n\u001b[0;32m    701\u001b[0m     index\u001b[39m=\u001b[39mindex,\n\u001b[0;32m    702\u001b[0m     index_label\u001b[39m=\u001b[39mindex_label,\n\u001b[0;32m    703\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[0;32m    704\u001b[0m     chunksize\u001b[39m=\u001b[39mchunksize,\n\u001b[0;32m    705\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    706\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    707\u001b[0m     engine\u001b[39m=\u001b[39mengine,\n\u001b[0;32m    708\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mengine_kwargs,\n\u001b[0;32m    709\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1726\u001b[0m, in \u001b[0;36mSQLDatabase.to_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method, engine, **engine_kwargs)\u001b[0m\n\u001b[0;32m   1676\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1677\u001b[0m \u001b[39mWrite records stored in a DataFrame to a SQL database.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[39m    Any additional kwargs are passed to the engine.\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m sql_engine \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m-> 1726\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_table(\n\u001b[0;32m   1727\u001b[0m     frame\u001b[39m=\u001b[39;49mframe,\n\u001b[0;32m   1728\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1729\u001b[0m     if_exists\u001b[39m=\u001b[39;49mif_exists,\n\u001b[0;32m   1730\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   1731\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   1732\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[0;32m   1733\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1734\u001b[0m )\n\u001b[0;32m   1736\u001b[0m total_inserted \u001b[39m=\u001b[39m sql_engine\u001b[39m.\u001b[39minsert_records(\n\u001b[0;32m   1737\u001b[0m     table\u001b[39m=\u001b[39mtable,\n\u001b[0;32m   1738\u001b[0m     con\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnectable,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1745\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mengine_kwargs,\n\u001b[0;32m   1746\u001b[0m )\n\u001b[0;32m   1748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_case_sensitive(name\u001b[39m=\u001b[39mname, schema\u001b[39m=\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:1625\u001b[0m, in \u001b[0;36mSQLDatabase.prep_table\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, dtype)\u001b[0m\n\u001b[0;32m   1613\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe type of \u001b[39m\u001b[39m{\u001b[39;00mcol\u001b[39m}\u001b[39;00m\u001b[39m is not a SQLAlchemy type\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1615\u001b[0m table \u001b[39m=\u001b[39m SQLTable(\n\u001b[0;32m   1616\u001b[0m     name,\n\u001b[0;32m   1617\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1623\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1624\u001b[0m )\n\u001b[1;32m-> 1625\u001b[0m table\u001b[39m.\u001b[39;49mcreate()\n\u001b[0;32m   1626\u001b[0m \u001b[39mreturn\u001b[39;00m table\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:839\u001b[0m, in \u001b[0;36mSQLTable.create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mif_exists\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not valid for if_exists\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    838\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_create()\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:825\u001b[0m, in \u001b[0;36mSQLTable._execute_create\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable\u001b[39m.\u001b[39mtometadata(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpd_sql\u001b[39m.\u001b[39mmeta)\n\u001b[1;32m--> 825\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable\u001b[39m.\u001b[39;49mcreate(bind\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpd_sql\u001b[39m.\u001b[39;49mconnectable)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\schema.py:950\u001b[0m, in \u001b[0;36mTable.create\u001b[1;34m(self, bind, checkfirst)\u001b[0m\n\u001b[0;32m    948\u001b[0m \u001b[39mif\u001b[39;00m bind \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m     bind \u001b[39m=\u001b[39m _bind_or_error(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 950\u001b[0m bind\u001b[39m.\u001b[39;49m_run_ddl_visitor(ddl\u001b[39m.\u001b[39;49mSchemaGenerator, \u001b[39mself\u001b[39;49m, checkfirst\u001b[39m=\u001b[39;49mcheckfirst)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:3123\u001b[0m, in \u001b[0;36mEngine._run_ddl_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   3121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_ddl_visitor\u001b[39m(\u001b[39mself\u001b[39m, visitorcallable, element, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   3122\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin() \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m-> 3123\u001b[0m         conn\u001b[39m.\u001b[39m_run_ddl_visitor(visitorcallable, element, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2119\u001b[0m, in \u001b[0;36mConnection._run_ddl_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_ddl_visitor\u001b[39m(\u001b[39mself\u001b[39m, visitorcallable, element, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2113\u001b[0m     \u001b[39m\"\"\"run a DDL visitor.\u001b[39;00m\n\u001b[0;32m   2114\u001b[0m \n\u001b[0;32m   2115\u001b[0m \u001b[39m    This method is only here so that the MockConnection can change the\u001b[39;00m\n\u001b[0;32m   2116\u001b[0m \u001b[39m    options given to the visitor so that \"checkfirst\" is skipped.\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m \n\u001b[0;32m   2118\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2119\u001b[0m     visitorcallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mtraverse_single(element)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\visitors.py:524\u001b[0m, in \u001b[0;36mExternalTraversal.traverse_single\u001b[1;34m(self, obj, **kw)\u001b[0m\n\u001b[0;32m    522\u001b[0m meth \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(v, \u001b[39m\"\u001b[39m\u001b[39mvisit_\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m obj\u001b[39m.\u001b[39m__visit_name__, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m meth:\n\u001b[1;32m--> 524\u001b[0m     \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py:874\u001b[0m, in \u001b[0;36mSchemaGenerator.visit_table\u001b[1;34m(self, table, create_ok, include_foreign_key_constraints, _is_metadata_operation)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisit_table\u001b[39m(\n\u001b[0;32m    868\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    869\u001b[0m     table,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m     _is_metadata_operation\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    873\u001b[0m ):\n\u001b[1;32m--> 874\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m create_ok \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_can_create_table(table):\n\u001b[0;32m    875\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    877\u001b[0m     table\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39mbefore_create(\n\u001b[0;32m    878\u001b[0m         table,\n\u001b[0;32m    879\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    882\u001b[0m         _is_metadata_operation\u001b[39m=\u001b[39m_is_metadata_operation,\n\u001b[0;32m    883\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py:787\u001b[0m, in \u001b[0;36mSchemaGenerator._can_create_table\u001b[1;34m(self, table)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_can_create_table\u001b[39m(\u001b[39mself\u001b[39m, table):\n\u001b[1;32m--> 787\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdialect\u001b[39m.\u001b[39;49mvalidate_identifier(table\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m    788\u001b[0m     effective_schema \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnection\u001b[39m.\u001b[39mschema_for_object(table)\n\u001b[0;32m    789\u001b[0m     \u001b[39mif\u001b[39;00m effective_schema:\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py:590\u001b[0m, in \u001b[0;36mDefaultDialect.validate_identifier\u001b[1;34m(self, ident)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate_identifier\u001b[39m(\u001b[39mself\u001b[39m, ident):\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(ident) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_identifier_length:\n\u001b[1;32m--> 590\u001b[0m         \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mIdentifierError(\n\u001b[0;32m    591\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIdentifier \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m exceeds maximum length of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m characters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    592\u001b[0m             \u001b[39m%\u001b[39m (ident, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_identifier_length)\n\u001b[0;32m    593\u001b[0m         )\n",
      "\u001b[1;31mIdentifierError\u001b[0m: Identifier 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),altura_oleaje float8, place text, time timestamp, url text, mag float8, lng float8, lat float8, deepth float8);' exceeds maximum length of 63 characters"
     ]
    }
   ],
   "source": [
    "df1=tsunami_Noaa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Sismos`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Los sismos ontenidos de la API de Noaa cumplen una o varias de las siguientes condiciones:***\n",
    "* Generaron un daño moderado (arpoximadamente 1 millon USD o más).\n",
    "* Produjeron 10 o más muertes.\n",
    "* Magnitud mayor a 7.5.\n",
    "* Intendidad del sismo en escala de Mercalli mayor o igual a X.\n",
    "* Produjo un tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sismos_Noaa():\n",
    "    ''' \n",
    "    Función:\n",
    "    Entrada:\n",
    "    Salida:\n",
    "    '''\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    countries = ['chile', 'japan', 'usa']\n",
    "    for country in countries:\n",
    "        \n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/earthquakes?country={country.upper()}&maxYear=2022&minYear=2000'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Creamos la columna con la fecha del sismo\n",
    "        df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']])\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'deepth', 'latitude':'lat', 'longitude':'lng', 'locationName':'place', 'country':'pais'}, inplace=True)\n",
    "\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        df['title'] = None\n",
    "        df['tsunami'] = df.apply(lambda x: 1 if pd.isnull(x.tsunamiEventId) != True else 0, axis=1)\n",
    "        if country=='japan':\n",
    "            country='japon' \n",
    "        df['idpais']  = paises(country)\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "        \n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','mag','place', 'time', 'url', 'tsunami', 'title', 'lng',\n",
    "            'lat', 'deepth']]\n",
    "        df['peligro']=1\n",
    "\n",
    "        #Info para terminar de cargar la tabla\n",
    "        # df=limpieza_general_tabla(df)\n",
    "        # #Paso la tabla a Potgres\n",
    "        df.to_sql(name=country,con=cone, if_exists='append', index=False)\n",
    "        df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "        print(f'La carga se ha hecho con exito!{country}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carga se ha hecho con exito!chile\n",
      "La carga se ha hecho con exito!japon\n",
      "La carga se ha hecho con exito!usa\n"
     ]
    }
   ],
   "source": [
    "sismos_Noaa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Volcanes`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultima erupción acorde con el program global de vulcanología del Smithsonian Institution.\n",
    "* D1\tUtima erupcion en 1964 o despues.\n",
    "* D2\tUtima erupcion entre 1900-1963.\n",
    "* D3\tUtima erupcion entre 1800-1899.\n",
    "* D4\tUtima erupcion entre 1700-1799\n",
    "* D5\tUtima erupcion entre 1500-1699\n",
    "* D6\tUtima erupcion entre A.D. 1-1499\n",
    "* D7\tUtima erupcion entre B.C. (Holocene)\n",
    "* U\tNo fechado, posiblemente en Holoceno\n",
    "* Q\tErupcion cuaternaria\n",
    "* ?\tErupción en el Holoceno incierta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volcanes_Noaa():\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    countries = ['chile', 'japan', 'United%20States']\n",
    "    for country in countries:\n",
    "        \n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanolocs?country={country.upper()}'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'name':'nombre', 'morphology':'tipo', 'elevation':'elevacion', 'latitude':'lat', 'longitude':'lng', 'location':'place', 'country':'pais', 'timeErupt':'ultima_erupcion'}, inplace=True)\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        df['idpais']  = df.apply(lambda x: 'Usa' if x.pais.lower() == 'united states' else x.pais,axis=1)\n",
    "        df['idpais']  = df.idpais.apply(lambda x: paises(x))\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "\n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','nombre','tipo', 'elevacion', 'place', 'ultima_erupcion', 'lat', 'lng','url']]\n",
    "    \n",
    "        display(df)\n",
    "        \n",
    "        df.to_sql(name='volcanes',con=cone, if_exists='append', index=False)\n",
    "\n",
    "        #Info para terminar de cargar la tabla\n",
    "        # #Paso la tabla a Potgres\n",
    "        # df.to_sql(name=volcanes,con=cone, if_exists='append', index=False)\n",
    "        # print('La carga se ha hecho con exito!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>nombre</th>\n",
       "      <th>tipo</th>\n",
       "      <th>elevacion</th>\n",
       "      <th>place</th>\n",
       "      <th>ultima_erupcion</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Tacora</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>5980.0</td>\n",
       "      <td>chile-n</td>\n",
       "      <td>U</td>\n",
       "      <td>-17.720</td>\n",
       "      <td>-69.770</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Caichinque</td>\n",
       "      <td>Maar</td>\n",
       "      <td>4458.0</td>\n",
       "      <td>chile-n</td>\n",
       "      <td>U</td>\n",
       "      <td>-23.949</td>\n",
       "      <td>-67.740</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Unnamed</td>\n",
       "      <td>Submarine volcano</td>\n",
       "      <td>-642.0</td>\n",
       "      <td>chile-is</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-33.620</td>\n",
       "      <td>-76.830</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Villarrica</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>2847.0</td>\n",
       "      <td>chile-c</td>\n",
       "      <td>D1</td>\n",
       "      <td>-39.420</td>\n",
       "      <td>-71.930</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yate</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>2187.0</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>U</td>\n",
       "      <td>-41.755</td>\n",
       "      <td>-72.396</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4</td>\n",
       "      <td>Cayutue-La Vigueria</td>\n",
       "      <td>Pyroclastic cones</td>\n",
       "      <td>506.0</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>D7</td>\n",
       "      <td>-41.250</td>\n",
       "      <td>-72.270</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>Corcovado</td>\n",
       "      <td>Stratovolcanoes</td>\n",
       "      <td>1826.0</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>D7</td>\n",
       "      <td>-43.189</td>\n",
       "      <td>-72.794</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>Río Murta</td>\n",
       "      <td>Pyroclastic cones</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>?</td>\n",
       "      <td>-46.167</td>\n",
       "      <td>-72.667</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>Arenales</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>3437.0</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>D2</td>\n",
       "      <td>-47.200</td>\n",
       "      <td>-73.483</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4</td>\n",
       "      <td>Meullin</td>\n",
       "      <td>Volcanic field</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>chile-s</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>-45.220</td>\n",
       "      <td>-73.050</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    idpais               nombre               tipo  elevacion     place  \\\n",
       "0        4               Tacora      Stratovolcano     5980.0   chile-n   \n",
       "1        4           Caichinque               Maar     4458.0   chile-n   \n",
       "2        4              Unnamed  Submarine volcano     -642.0  chile-is   \n",
       "3        4           Villarrica      Stratovolcano     2847.0   chile-c   \n",
       "4        4                 Yate      Stratovolcano     2187.0   chile-s   \n",
       "..     ...                  ...                ...        ...       ...   \n",
       "83       4  Cayutue-La Vigueria  Pyroclastic cones      506.0   chile-s   \n",
       "84       4            Corcovado    Stratovolcanoes     1826.0   chile-s   \n",
       "85       4            Río Murta  Pyroclastic cones        NaN   chile-s   \n",
       "86       4             Arenales      Stratovolcano     3437.0   chile-s   \n",
       "87       4              Meullin     Volcanic field     1080.0   chile-s   \n",
       "\n",
       "   ultima_erupcion     lat     lng  \\\n",
       "0                U -17.720 -69.770   \n",
       "1                U -23.949 -67.740   \n",
       "2          Unknown -33.620 -76.830   \n",
       "3               D1 -39.420 -71.930   \n",
       "4                U -41.755 -72.396   \n",
       "..             ...     ...     ...   \n",
       "83              D7 -41.250 -72.270   \n",
       "84              D7 -43.189 -72.794   \n",
       "85               ? -46.167 -72.667   \n",
       "86              D2 -47.200 -73.483   \n",
       "87         Unknown -45.220 -73.050   \n",
       "\n",
       "                                                  url  \n",
       "0   https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "1   https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "2   https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "3   https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "4   https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "..                                                ...  \n",
       "83  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "84  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "85  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "86  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "87  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>nombre</th>\n",
       "      <th>tipo</th>\n",
       "      <th>elevacion</th>\n",
       "      <th>place</th>\n",
       "      <th>ultima_erupcion</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Suwanosejima</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>796</td>\n",
       "      <td>ryukyu is</td>\n",
       "      <td>D1</td>\n",
       "      <td>29.638</td>\n",
       "      <td>129.714</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Takaharayama</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>1795</td>\n",
       "      <td>honshu-japan</td>\n",
       "      <td>U7</td>\n",
       "      <td>36.900</td>\n",
       "      <td>139.777</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Midagahara</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>2621</td>\n",
       "      <td>honshu-japan</td>\n",
       "      <td>D3</td>\n",
       "      <td>36.571</td>\n",
       "      <td>137.590</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Yokodake</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>2480</td>\n",
       "      <td>honshu-japan</td>\n",
       "      <td>U</td>\n",
       "      <td>36.087</td>\n",
       "      <td>138.320</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Toshima</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>508</td>\n",
       "      <td>izu is-japan</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>34.520</td>\n",
       "      <td>139.279</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5</td>\n",
       "      <td>Kaikata Seamount</td>\n",
       "      <td>Submarine volcano</td>\n",
       "      <td>-165</td>\n",
       "      <td>volcano is-japan</td>\n",
       "      <td>U</td>\n",
       "      <td>26.667</td>\n",
       "      <td>140.929</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>5</td>\n",
       "      <td>Unnamed</td>\n",
       "      <td>Submarine volcano?</td>\n",
       "      <td>-3200</td>\n",
       "      <td>volcano is-japan</td>\n",
       "      <td>?</td>\n",
       "      <td>26.133</td>\n",
       "      <td>144.483</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>5</td>\n",
       "      <td>Daikoku</td>\n",
       "      <td>Submarine volcano</td>\n",
       "      <td>-323</td>\n",
       "      <td>volcano is-japan</td>\n",
       "      <td>U</td>\n",
       "      <td>21.324</td>\n",
       "      <td>144.194</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>5</td>\n",
       "      <td>Akandanayama</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>2109</td>\n",
       "      <td>honshu-japan</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>36.200</td>\n",
       "      <td>137.573</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>5</td>\n",
       "      <td>Tenchozan</td>\n",
       "      <td>Crater rows</td>\n",
       "      <td>1046</td>\n",
       "      <td>hokkaido-japan</td>\n",
       "      <td>?</td>\n",
       "      <td>44.044</td>\n",
       "      <td>145.086</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idpais            nombre                tipo  elevacion  \\\n",
       "0         5      Suwanosejima       Stratovolcano        796   \n",
       "1         5      Takaharayama       Stratovolcano       1795   \n",
       "2         5        Midagahara       Stratovolcano       2621   \n",
       "3         5          Yokodake       Stratovolcano       2480   \n",
       "4         5           Toshima       Stratovolcano        508   \n",
       "..      ...               ...                 ...        ...   \n",
       "108       5  Kaikata Seamount   Submarine volcano       -165   \n",
       "109       5           Unnamed  Submarine volcano?      -3200   \n",
       "110       5           Daikoku   Submarine volcano       -323   \n",
       "111       5      Akandanayama       Stratovolcano       2109   \n",
       "112       5         Tenchozan         Crater rows       1046   \n",
       "\n",
       "                place ultima_erupcion     lat      lng  \\\n",
       "0           ryukyu is              D1  29.638  129.714   \n",
       "1        honshu-japan              U7  36.900  139.777   \n",
       "2        honshu-japan              D3  36.571  137.590   \n",
       "3        honshu-japan               U  36.087  138.320   \n",
       "4        izu is-japan         Unknown  34.520  139.279   \n",
       "..                ...             ...     ...      ...   \n",
       "108  volcano is-japan               U  26.667  140.929   \n",
       "109  volcano is-japan               ?  26.133  144.483   \n",
       "110  volcano is-japan               U  21.324  144.194   \n",
       "111      honshu-japan         Unknown  36.200  137.573   \n",
       "112    hokkaido-japan               ?  44.044  145.086   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "1    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "2    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "3    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "4    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "..                                                 ...  \n",
       "108  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "109  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "110  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "111  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "112  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "\n",
       "[113 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idpais</th>\n",
       "      <th>nombre</th>\n",
       "      <th>tipo</th>\n",
       "      <th>elevacion</th>\n",
       "      <th>place</th>\n",
       "      <th>ultima_erupcion</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>San Francisco Volcanic Field</td>\n",
       "      <td>Cinder cone</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>us-arizona</td>\n",
       "      <td>D6</td>\n",
       "      <td>35.347</td>\n",
       "      <td>-111.678</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>Supply Reef</td>\n",
       "      <td>Submarine volcano</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>mariana is-c pacific</td>\n",
       "      <td>D1</td>\n",
       "      <td>20.130</td>\n",
       "      <td>145.100</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Ta'u</td>\n",
       "      <td>Shield volcano</td>\n",
       "      <td>931.0</td>\n",
       "      <td>samoa-sw pacific</td>\n",
       "      <td>U</td>\n",
       "      <td>-14.230</td>\n",
       "      <td>-169.454</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Takawangha</td>\n",
       "      <td>Stratovolcano</td>\n",
       "      <td>1449.0</td>\n",
       "      <td>aleutian is</td>\n",
       "      <td>U</td>\n",
       "      <td>51.873</td>\n",
       "      <td>-178.006</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Tlevak Strait-Suemez Island</td>\n",
       "      <td>Volcanic field</td>\n",
       "      <td>50.0</td>\n",
       "      <td>alaska-se</td>\n",
       "      <td>U</td>\n",
       "      <td>55.250</td>\n",
       "      <td>-133.300</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>6</td>\n",
       "      <td>Mariana Back-Arc Segment at 15.5°N</td>\n",
       "      <td>Submarine</td>\n",
       "      <td>-4100.0</td>\n",
       "      <td>mariana is-c pacific</td>\n",
       "      <td>D1</td>\n",
       "      <td>15.406</td>\n",
       "      <td>144.506</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>6</td>\n",
       "      <td>Korovin</td>\n",
       "      <td>Stratovolcano(es)</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>aleutian is</td>\n",
       "      <td>D1</td>\n",
       "      <td>52.381</td>\n",
       "      <td>-174.166</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>6</td>\n",
       "      <td>St. Paul Island</td>\n",
       "      <td>Shield</td>\n",
       "      <td>203.0</td>\n",
       "      <td>alaska-w</td>\n",
       "      <td>D7</td>\n",
       "      <td>57.180</td>\n",
       "      <td>-170.300</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>6</td>\n",
       "      <td>Red Hill</td>\n",
       "      <td>Volcanic field</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>us-new mexico</td>\n",
       "      <td>D7</td>\n",
       "      <td>34.250</td>\n",
       "      <td>-108.830</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>6</td>\n",
       "      <td>Stepovak Bay Group</td>\n",
       "      <td>Volcanic field</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>alaska peninsula</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>55.917</td>\n",
       "      <td>-160.017</td>\n",
       "      <td>https://www.ngdc.noaa.gov/hazel/hazard-service...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     idpais                              nombre               tipo  elevacion  \\\n",
       "0         6        San Francisco Volcanic Field        Cinder cone     3850.0   \n",
       "1         6                         Supply Reef  Submarine volcano       -8.0   \n",
       "2         6                                Ta'u     Shield volcano      931.0   \n",
       "3         6                          Takawangha      Stratovolcano     1449.0   \n",
       "4         6         Tlevak Strait-Suemez Island     Volcanic field       50.0   \n",
       "..      ...                                 ...                ...        ...   \n",
       "185       6  Mariana Back-Arc Segment at 15.5°N          Submarine    -4100.0   \n",
       "186       6                             Korovin  Stratovolcano(es)     1518.0   \n",
       "187       6                     St. Paul Island             Shield      203.0   \n",
       "188       6                            Red Hill     Volcanic field     2300.0   \n",
       "189       6                  Stepovak Bay Group     Volcanic field     1633.0   \n",
       "\n",
       "                    place ultima_erupcion     lat      lng  \\\n",
       "0              us-arizona              D6  35.347 -111.678   \n",
       "1    mariana is-c pacific              D1  20.130  145.100   \n",
       "2        samoa-sw pacific               U -14.230 -169.454   \n",
       "3             aleutian is               U  51.873 -178.006   \n",
       "4               alaska-se               U  55.250 -133.300   \n",
       "..                    ...             ...     ...      ...   \n",
       "185  mariana is-c pacific              D1  15.406  144.506   \n",
       "186           aleutian is              D1  52.381 -174.166   \n",
       "187              alaska-w              D7  57.180 -170.300   \n",
       "188         us-new mexico              D7  34.250 -108.830   \n",
       "189      alaska peninsula         Unknown  55.917 -160.017   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "1    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "2    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "3    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "4    https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "..                                                 ...  \n",
       "185  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "186  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "187  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "188  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "189  https://www.ngdc.noaa.gov/hazel/hazard-service...  \n",
       "\n",
       "[190 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "volcanes_Noaa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852b27bee67d373d5d537f6ee9474570f8f7363f24457c4522e6a7b2aeb81c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
