{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "#Para rellenar vacíos\n",
    "import numpy as np\n",
    "\n",
    "#ETL:\n",
    "\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#para normalizar fechas\n",
    "import datetime\n",
    "\n",
    "#Conexión con postgresql:\n",
    "\n",
    "#Para crear tablas con claves primarias y foraneas\n",
    "import psycopg2\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Carga de datos a postgres:\n",
    "\n",
    "#Para tener errores en try y except\n",
    "import traceback\n",
    "\n",
    "#Ver para japón: (borrar si no se usa)\n",
    "from urllib.parse import quote\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`ETL`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_index(df):\n",
    "    '''\n",
    "    Normalizamos índice\n",
    "    '''\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla (df):\n",
    "    '''\n",
    "    Función limpieza de cadenas de string\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace = True) \n",
    "    #Acomodamos el indice\n",
    "    df=normalizar_index(df)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)                  \n",
    "        #sacamos los acentos\n",
    "        df[c]=df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c== 'place':\n",
    " \n",
    "            lista_simbolos=[' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c]=df[c].apply(lambda x: 'sin dato' if type(x)== str and x=='' else x)\n",
    "            #sacamos los que no tengan el pais que buscamos\n",
    "            df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')] \n",
    "            #los eliminamos de place\n",
    "            lista_simbolos=['japan','chile']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)\n",
    "\n",
    "        #detectamos NaN\n",
    "        df[c]=df[c].apply(lambda x: None if type(x)== str and x=='' else x)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\\n\\nfrom datetime import datetime\\nimport pytz\\n\\nlocal = datetime.now()\\nprint(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n\\ntz_NY = pytz.timezone(\\'America/New_York\\') \\ndatetime_NY = datetime.now(tz_NY)\\nprint(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "INTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "local = datetime.now()\n",
    "print(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_NY = datetime.now(tz_NY)\n",
    "print(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos (url,pais):\n",
    "    '''\n",
    "    Función para pasar de los datos de url de USGS\n",
    "    Toma de parámetro una url y devuelve el df que utilizaremos\n",
    "\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    for i, fila in enumerate(resp['features']):\n",
    "        \n",
    "        a=resp['features'][i]\n",
    "        #agrego al diccionario magnitud, tiempo, si produjo tsunami y sig\n",
    "        lista_propierties=['mag','time','tsunami','sig']\n",
    "        for elemento in lista_propierties:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(np.nan) \n",
    "            else:\n",
    "                dict[elemento].append(float(a['properties'][elemento]))\n",
    "                \n",
    "        \n",
    "        #agrego lugar, url con información a ampliar y el título     \n",
    "        lista_propierties2=['place','url','title']\n",
    "        for elemento in lista_propierties2:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append('Sin dato')\n",
    "            else:\n",
    "                dict[elemento].append(a['properties'][elemento])\n",
    "\n",
    "        #Agrego al diccionario latitud, longitud y profundidad\n",
    "        list_geometry=['lng','lat','deepth']\n",
    "        for indice, elemento in enumerate(list_geometry):\n",
    "            if a['geometry']['coordinates'][indice] is None:\n",
    "                dict[elemento].append(np.nan)\n",
    "            else:\n",
    "                dict[elemento].append(float(a['geometry']['coordinates'][indice]))\n",
    "\n",
    "    #Devuelvo el diccionario hecho df\n",
    "    return (pd.DataFrame(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultimoDia(anio, mes):\n",
    "    '''\n",
    "    Función ultimo día del mes\n",
    "    Devuelve el último día del mes\n",
    "\n",
    "    anio: año a procesar\n",
    "    mes: mes del cual obtener el último día\n",
    "    '''\n",
    "    #Valor por defecto\n",
    "    ultimo_dia = 31\n",
    "    #Meses con 30 días\n",
    "    if mes in [4, 6, 9, 11]:\n",
    "        ultimo_dia = 30\n",
    "\n",
    "    # Vemos si el año es bisiesto\n",
    "    if mes == 2:\n",
    "        if (anio % 4) == 0:\n",
    "            if (anio % 100) == 0:\n",
    "                if (anio % 400) == 0:\n",
    "                    ultimo_dia = 29\n",
    "                else:\n",
    "                    ultimo_dia = 28\n",
    "            else:\n",
    "                ultimo_dia = 29\n",
    "        else:\n",
    "            ultimo_dia = 28\n",
    "\n",
    "    return ultimo_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Carga (Load - Potgresql)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexión a postgres mediante alquemy\n",
    "cone = create_engine('postgresql://sismosu:123@localhost:5432/sismosdb', pool_size=50, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "\n",
    "tabla_eeuu_usgs = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_chile_usgs = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_japon_usgs = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (id SERIAL PRIMARY KEY NOT NULL  ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='sismosu',\n",
    "    password='123',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(tabla_eeuu_usgs)\n",
    "cur.execute(tabla_chile_usgs)\n",
    "cur.execute(tabla_japon_usgs)\n",
    "cur.execute(tabla_hechos)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Japón`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"anio=2020\\nmes=5\\nfecha_desde = f'{str(anio)}-{str(mes)}-01'\\nfecha_hasta = f'{str(anio)}-{str(mes + 1)}-01'\\nurl = f'http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&minimumlength=30&start=2018-11-01T20:12:12&end=2018-11-01T20:35:33&format=geocsv.slist.inline&nodata=404'\\nr = requests.get(url)\\ndf = pd.read_csv(io.StringIO(r.text))\\n#corregimos las fechas\\n#df=limpieza_fechas(df)\\n#normalizamos cadenas de string\\n#df=limpieza_general_tabla(df)\\n#Paso la tabla a Potgres\\ndf.head()\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''anio=2020\n",
    "mes=5\n",
    "fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "fecha_hasta = f'{str(anio)}-{str(mes + 1)}-01'\n",
    "url = f'http://service.iris.edu/fdsnws/dataselect/1/query?net=IU&minimumlength=30&start=2018-11-01T20:12:12&end=2018-11-01T20:35:33&format=geocsv.slist.inline&nodata=404'\n",
    "r = requests.get(url)\n",
    "df = pd.read_csv(io.StringIO(r.text))\n",
    "#corregimos las fechas\n",
    "#df=limpieza_fechas(df)\n",
    "#normalizamos cadenas de string\n",
    "#df=limpieza_general_tabla(df)\n",
    "#Paso la tabla a Potgres\n",
    "df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.index.get_level_values(0)[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"url = f'service.iris.edu/fdsnws/event/1/query?eventid={3022}'\\nr = requests.get(url)\\ndf2 = pd.read_csv(io.StringIO(r.text))\\ndf2\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''url = f'service.iris.edu/fdsnws/event/1/query?eventid={3022}'\n",
    "r = requests.get(url)\n",
    "df2 = pd.read_csv(io.StringIO(r.text))\n",
    "df2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"url = 'http://service.iris.edu/fdsnws/event/1/query?originid=3022'\\nr = requests.get(url)\\ndf2 = pd.read_csv(io.StringIO(r.text))\\ndf2\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''url = 'http://service.iris.edu/fdsnws/event/1/query?originid=3022'\n",
    "r = requests.get(url)\n",
    "df2 = pd.read_csv(io.StringIO(r.text))\n",
    "df2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df['Error 400: Bad Request'][0]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df['Error 400: Bad Request'][0]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Carga de datos de USGS`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def cargar_paises(anio,mes,dia_fin):\n",
    "    \n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en tres tablas distintas\n",
    "    Sirve para respaldo\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['usa', 'japon', 'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "    error=[]\n",
    "    for pais in paises:\n",
    "        while anio > 1999:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name=pais,con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta)\n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "                error.append(traceback.print_exc())\n",
    "            # Decrementamos el mes\n",
    "            if mes == 1:\n",
    "                # Reducimos el año\n",
    "                anio -= 1\n",
    "                mes = 12\n",
    "            else:\n",
    "                mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "    errores['error']=error\n",
    "    display(errores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "#Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises(anio,mes,dia_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=limpieza_general_tabla(df)\n",
    "df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')]\n",
    "\n",
    "ERROR: raise AttributeError(\"Can only use .str accessor with string values!\")\n",
    "\n",
    "Error en la carga en la fecha: 2020-6-01 hasta 2020-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2019-7-01 hasta 2019-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-7-01 hasta 2018-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-6-01 hasta 2018-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2010-4-01 hasta 2010-4-30 del país: usa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\\ndf['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\\ndf['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\\ndf['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\\ndf['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\\ndf['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\n",
    "df['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\n",
    "df['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\n",
    "df['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\n",
    "df['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\n",
    "df['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_paises_sismos(anio,mes,dia_fin):\n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en una única tabla de hechos\n",
    "    Sirve para automatizar la carga incremental\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['japon', 'usa' ,'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "\n",
    "    while anio > 2000:\n",
    "        #recorre la lista de paises\n",
    "        for pais in paises:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Le decimos que es formato fecha\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "            \n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "\n",
    "\n",
    "        # Decrementamos el mes\n",
    "        if mes == 1:\n",
    "            # Reducimos el año\n",
    "            anio -= 1\n",
    "            mes = 12\n",
    "        else:\n",
    "            mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "\n",
    "    display(errores)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "# Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises_sismos(anio,mes,dia_fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852b27bee67d373d5d537f6ee9474570f8f7363f24457c4522e6a7b2aeb81c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
