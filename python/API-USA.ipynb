{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "#Para rellenar vacíos\n",
    "import numpy as np\n",
    "import json\n",
    "#ETL:\n",
    "\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#para normalizar fechas\n",
    "import datetime\n",
    "\n",
    "#Conexión con postgresql:\n",
    "\n",
    "#Para crear tablas con claves primarias y foraneas\n",
    "import psycopg2\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Carga de datos a postgres:\n",
    "\n",
    "#Para tener errores en try y except\n",
    "import traceback\n",
    "\n",
    "#Ver para japón: (borrar si no se usa)\n",
    "from urllib.parse import quote\n",
    "import io\n",
    "\n",
    "#limpiar la consola\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`ETL`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_index(df):\n",
    "    '''\n",
    "    Normalizamos índice\n",
    "    '''\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla (df):\n",
    "    '''\n",
    "    Función limpieza de cadenas de string\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace = True) \n",
    "    #Acomodamos el indice\n",
    "    df=normalizar_index(df)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)                  \n",
    "        #sacamos los acentos\n",
    "        df[c]=df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c== 'place':\n",
    " \n",
    "            lista_simbolos=[' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c]=df[c].apply(lambda x: 'sin dato' if type(x)== str and x=='' else x)\n",
    "            #sacamos los que no tengan el pais que buscamos\n",
    "            df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')] \n",
    "            #los eliminamos de place\n",
    "            lista_simbolos=['japan','chile']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)\n",
    "\n",
    "        #detectamos NaN\n",
    "        df[c]=df[c].apply(lambda x: None if type(x)== str and x=='' else x)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\\n\\nfrom datetime import datetime\\nimport pytz\\n\\nlocal = datetime.now()\\nprint(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n\\ntz_NY = pytz.timezone(\\'America/New_York\\') \\ndatetime_NY = datetime.now(tz_NY)\\nprint(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "INTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "local = datetime.now()\n",
    "print(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_NY = datetime.now(tz_NY)\n",
    "print(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos (url,pais):\n",
    "    '''\n",
    "    Función para pasar de los datos de url de USGS\n",
    "    Toma de parámetro una url y devuelve el df que utilizaremos\n",
    "\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    for i, fila in enumerate(resp['features']):\n",
    "        \n",
    "        a=resp['features'][i]\n",
    "        #agrego al diccionario magnitud, tiempo, si produjo tsunami y sig\n",
    "        lista_propierties=['mag','time','tsunami','sig']\n",
    "        for elemento in lista_propierties:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(np.nan) \n",
    "            else:\n",
    "                dict[elemento].append(float(a['properties'][elemento]))\n",
    "                \n",
    "        \n",
    "        #agrego lugar, url con información a ampliar y el título     \n",
    "        lista_propierties2=['place','url','title']\n",
    "        for elemento in lista_propierties2:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append('Sin dato')\n",
    "            else:\n",
    "                dict[elemento].append(a['properties'][elemento])\n",
    "\n",
    "        #Agrego al diccionario latitud, longitud y profundidad\n",
    "        list_geometry=['lng','lat','deepth']\n",
    "        for indice, elemento in enumerate(list_geometry):\n",
    "            if a['geometry']['coordinates'][indice] is None:\n",
    "                dict[elemento].append(np.nan)\n",
    "            else:\n",
    "                dict[elemento].append(float(a['geometry']['coordinates'][indice]))\n",
    "\n",
    "    #Devuelvo el diccionario hecho df\n",
    "    return (pd.DataFrame(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultimoDia(anio, mes):\n",
    "    '''\n",
    "    Función ultimo día del mes\n",
    "    Devuelve el último día del mes\n",
    "\n",
    "    anio: año a procesar\n",
    "    mes: mes del cual obtener el último día\n",
    "    '''\n",
    "    #Valor por defecto\n",
    "    ultimo_dia = 31\n",
    "    #Meses con 30 días\n",
    "    if mes in [4, 6, 9, 11]:\n",
    "        ultimo_dia = 30\n",
    "\n",
    "    # Vemos si el año es bisiesto\n",
    "    if mes == 2:\n",
    "        if (anio % 4) == 0:\n",
    "            if (anio % 100) == 0:\n",
    "                if (anio % 400) == 0:\n",
    "                    ultimo_dia = 29\n",
    "                else:\n",
    "                    ultimo_dia = 28\n",
    "            else:\n",
    "                ultimo_dia = 29\n",
    "        else:\n",
    "            ultimo_dia = 28\n",
    "\n",
    "    return ultimo_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Carga (Load - Potgresql)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexión a postgres mediante alquemy\n",
    "cone = create_engine('postgresql://sismosu:123@localhost:5432/sismosdb', pool_size=50, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gise\\Desktop\\Data Since\\LABS\\Grupal\\usgsAPI\\pythonPrograms\\script\\API-USA.ipynb Celda 10\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tabla_japon_usgs \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (id SERIAL PRIMARY KEY NOT NULL  ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tabla_hechos \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m conn \u001b[39m=\u001b[39m psycopg2\u001b[39m.\u001b[39;49mconnect(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     host\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlocalhost\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     user\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msismosu\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m123\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     database\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msismosdb\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     port\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m5432\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m  )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m cur \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Gise/Desktop/Data%20Since/LABS/Grupal/usgsAPI/pythonPrograms/script/API-USA.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m cur\u001b[39m.\u001b[39mexecute(tabla_eeuu_usgs)\n",
      "File \u001b[1;32mc:\\Users\\Gise\\anaconda3\\lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39masync_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39mconnection_factory, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwasync)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[39m.\u001b[39mcursor_factory \u001b[39m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: connection to server at \"localhost\" (::1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\nconnection to server at \"localhost\" (127.0.0.1), port 5432 failed: Connection refused (0x0000274D/10061)\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "\n",
    "tabla_eeuu_usgs = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_chile_usgs = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_japon_usgs = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (id SERIAL PRIMARY KEY NOT NULL  ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='sismosu',\n",
    "    password='123',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(tabla_eeuu_usgs)\n",
    "cur.execute(tabla_chile_usgs)\n",
    "cur.execute(tabla_japon_usgs)\n",
    "cur.execute(tabla_hechos)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Japón`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explcación de parámetros de la api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "límite\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "número de devoluciones (1-100, el valor predeterminado es 10)\n",
    "'''\n",
    "limit=10\n",
    "'''\n",
    "compensar\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "Número de registros para omitir \n",
    "(0 o más, el valor predeterminado es 0). \n",
    "Por ejemplo, limit=100&offset=200 devuelve 100 registros desde el 201.\n",
    "'''\n",
    "offset=200\n",
    "'''\n",
    "ordenar\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "Orden de clasificación. 1 es orden ascendente (primero el más antiguo), \n",
    "-1 es orden descendente (primero el más nuevo). \n",
    "El valor predeterminado es el orden descendente.\n",
    "'''\n",
    "order=-1\n",
    "'''\n",
    "desde_fecha\n",
    "cuerda\n",
    "( consulta )\t\n",
    "Fecha especificada o posterior (formato aaaaMMdd)\n",
    "'''\n",
    "\n",
    "fecha_start=20171121\n",
    "'''\n",
    "tipo_terremoto\n",
    "cuerda\n",
    "( consulta )\t\n",
    "Tipo de información del terremoto. \n",
    "Los valores son ScalePrompt (informe preliminar de intensidad sísmica), \n",
    "Destination (información sobre la fuente del terremoto), \n",
    "ScaleAndDestination (información sobre la intensidad sísmica y el epicentro), \n",
    "DetailScale (información sobre la intensidad sísmica de cada lugar), \n",
    "Foreign (información sobre el terremoto lejano), \n",
    "Otro (otra información).\n",
    "'''\n",
    "quake_type='ScaleAndDestination'\n",
    "\n",
    "'''\n",
    "min_magnitud/max_magnitude\n",
    "número\n",
    "( consulta )\t\n",
    "límite de magnitud inferior/superior\n",
    "'''\n",
    "\n",
    "min_magnitude=1\n",
    "max_magnitude=9\n",
    "\n",
    "'''\n",
    "min_escala\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "Límite inferior de máxima intensidad sísmica. \n",
    "Los valores son 10 (intensidad sísmica 1),\n",
    "20 (intensidad sísmica 2), \n",
    "30 (intensidad sísmica 3), \n",
    "40 (intensidad sísmica 4), \n",
    "45 (intensidad sísmica 5 inferior), \n",
    "50 (intensidad sísmica 5 superior), \n",
    "55 (intensidad sísmica 6 menor), \n",
    "60 (intensidad sísmica 6-fuerte), \n",
    "70 (intensidad sísmica 7).\n",
    "'''\n",
    "min_scale=10\n",
    "max_scale=70\n",
    "'''\n",
    "prefecturas[]\n",
    "matriz [objeto]\n",
    "( consulta )\t\n",
    "Intensidad sísmica mínima para cada prefectura. Especifique como \"Prefectura de Hyogo, 10\".\n",
    "'''\n",
    "#prefectures%5B%5D=\n",
    "\n",
    "#urlP2p = \"https://api.p2pquake.net/v2/jma/quake?limit={limit}&offset={offset}&order={order}&since_date={fecha_start}&quake_type={quake_type}&min_magnitude={min_magnitude}&max_magnitude={max_magnitude}&min_scale={min_scal}&max_scale={max_scale}&prefectures%5B%5D=\"\n",
    "urlP2p = \"https://api.p2pquake.net/v1/human-readable?limit=99&since_date={fecha_start}\"\n",
    "responseP2p = requests.get(urlP2p)\n",
    "responseP2p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Es este: Si queda lo pongo en una función para que sea llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022/11/21 00:00:00.000000\n",
      "<Response [200]>\n",
      "100 20221109\n",
      "<Response [200]>\n",
      "200 20221109\n",
      "<Response [200]>\n",
      "300 20221109\n",
      "<Response [200]>\n",
      "400 20221109\n",
      "<Response [200]>\n",
      "500 20221109\n",
      "<Response [200]>\n",
      "600 20221109\n",
      "<Response [200]>\n",
      "700 20221109\n",
      "<Response [200]>\n",
      "800 20221109\n",
      "<Response [200]>\n",
      "900 20221109\n",
      "fin\n",
      "20221109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>sig</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>deepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.7</td>\n",
       "      <td>小笠原村</td>\n",
       "      <td>2022/11/21 13:32:06.508</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>鳥島近海</td>\n",
       "      <td>E139.8</td>\n",
       "      <td>N29.2</td>\n",
       "      <td>430km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.3</td>\n",
       "      <td>いわき市</td>\n",
       "      <td>2022/11/21 05:23:39.972</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>福島県沖</td>\n",
       "      <td>E141.2</td>\n",
       "      <td>N37.1</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.3</td>\n",
       "      <td>None</td>\n",
       "      <td>2022/11/21 05:21:58.638</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>福島県沖</td>\n",
       "      <td>E141.2</td>\n",
       "      <td>N37.1</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.2</td>\n",
       "      <td>?a?o?s</td>\n",
       "      <td>2022/11/21 05:12:20.304</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>?a?o”?“‡“i“?‰≪</td>\n",
       "      <td>E146.4</td>\n",
       "      <td>N43.4</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.2</td>\n",
       "      <td>根室市</td>\n",
       "      <td>2022/11/21 05:12:19.949</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>根室半島南東沖</td>\n",
       "      <td>E146.4</td>\n",
       "      <td>N43.4</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.4</td>\n",
       "      <td>?}?O?s</td>\n",
       "      <td>2022/11/09 22:55:36.525</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>?i?e?§“i?”</td>\n",
       "      <td>E140.1</td>\n",
       "      <td>N36.0</td>\n",
       "      <td>60km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.4</td>\n",
       "      <td>笠間市</td>\n",
       "      <td>2022/11/09 22:55:36.248</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>茨城県南部</td>\n",
       "      <td>E140.1</td>\n",
       "      <td>N36.0</td>\n",
       "      <td>60km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>7.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2022/11/09 19:21:00.647</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>フィジー諸島南方</td>\n",
       "      <td>E178.6</td>\n",
       "      <td>S26.1</td>\n",
       "      <td>670km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5.0</td>\n",
       "      <td>城里町</td>\n",
       "      <td>2022/11/09 17:44:27.452</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>茨城県南部</td>\n",
       "      <td>E140.0</td>\n",
       "      <td>N36.2</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2022/11/09 17:43:01.394</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>茨城県南部</td>\n",
       "      <td>E140.0</td>\n",
       "      <td>N36.2</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mag   place                     time  \\\n",
       "0   5.7    小笠原村  2022/11/21 13:32:06.508   \n",
       "1   4.3    いわき市  2022/11/21 05:23:39.972   \n",
       "2   4.3    None  2022/11/21 05:21:58.638   \n",
       "5   3.2  ?a?o?s  2022/11/21 05:12:20.304   \n",
       "6   3.2     根室市  2022/11/21 05:12:19.949   \n",
       "..  ...     ...                      ...   \n",
       "92  3.4  ?}?O?s  2022/11/09 22:55:36.525   \n",
       "93  3.4     笠間市  2022/11/09 22:55:36.248   \n",
       "95  7.0    None  2022/11/09 19:21:00.647   \n",
       "96  5.0     城里町  2022/11/09 17:44:27.452   \n",
       "97  5.0    None  2022/11/09 17:43:01.394   \n",
       "\n",
       "                                                  url tsunami  sig  \\\n",
       "0   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "1   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "2   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "5   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "6   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "..                                                ...     ...  ...   \n",
       "92  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "93  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "95  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "96  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "97  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "\n",
       "             title     lng    lat deepth  \n",
       "0             鳥島近海  E139.8  N29.2  430km  \n",
       "1             福島県沖  E141.2  N37.1   50km  \n",
       "2             福島県沖  E141.2  N37.1   50km  \n",
       "5   ?a?o”?“‡“i“?‰≪  E146.4  N43.4   50km  \n",
       "6          根室半島南東沖  E146.4  N43.4   50km  \n",
       "..             ...     ...    ...    ...  \n",
       "92      ?i?e?§“i?”  E140.1  N36.0   60km  \n",
       "93           茨城県南部  E140.1  N36.0   60km  \n",
       "95        フィジー諸島南方  E178.6  S26.1  670km  \n",
       "96           茨城県南部  E140.0  N36.2   50km  \n",
       "97           茨城県南部  E140.0  N36.2   50km  \n",
       "\n",
       "[67 rows x 10 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creo dicc vacio\n",
    "dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "'''\n",
    "límite\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "número de devoluciones (1-100, el valor predeterminado es 10)\n",
    "'''\n",
    "limit=100\n",
    "'''\n",
    "compensar\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "Número de registros para omitir \n",
    "(0 o más, el valor predeterminado es 0). \n",
    "Por ejemplo, limit=100&offset=200 devuelve 100 registros desde el 201.\n",
    "'''\n",
    "offset=0\n",
    "'''\n",
    "desde_fecha\n",
    "cuerda\n",
    "( consulta )\t\n",
    "Fecha especificada o posterior (formato aaaaMMdd)\n",
    "'''\n",
    "\n",
    "#fecha_start=time.time()\n",
    "fecha_start=20180501\n",
    "#print(datetime.datetime.strptime(fecha_start, '%Y/%m/%d %H:%M:%S'))\n",
    "print(datetime.date.today().strftime('%Y/%m/%d %H:%M:%S.%f'))\n",
    "\n",
    "'''\n",
    "fecha_end\n",
    "límite de las iteraciones\n",
    "\n",
    "'''\n",
    "fecha_end= 20150406\n",
    "#fecha_end= str(20210801)\n",
    "#fecha_end=datetime.datetime.strptime(fecha_end, '%Y%m%d')\n",
    "\n",
    "\n",
    "    \n",
    "while offset<=2100 and fecha_start>=fecha_end :\n",
    "    urlP2p = \"https://api.p2pquake.net/v1/human-readable?limit=100&offset={offset}&since_date={fecha_start}\"\n",
    "    #urlP2p = \"https://api.p2pquake.net/v1/human-readable?limit=100&offset={offset}\"\n",
    "    responseP2p = requests.get(urlP2p)\n",
    "    jsonDataP2p = responseP2p.json()\n",
    "    for i in range(limit):\n",
    "        try:\n",
    "            try:a=jsonDataP2p[i]\n",
    "            except:pass\n",
    "            \n",
    "            try:dict['mag'].append(a['earthquake']['hypocenter']['magnitude'])\n",
    "            except:dict['mag'].append(None)\n",
    "\n",
    "            try:dict['place'].append(a['points'][0]['addr'])\n",
    "            except:dict['place'].append(None)\n",
    "            \n",
    "            #try:dict['time'].append(datetime.datetime.strftime(a['time'], '%Y/%m/%d %H:%M:%S.%f'))            \n",
    "            try:dict['time'].append(a['time'])\n",
    "            except:dict['time'].append(None)\n",
    "            \n",
    "            try:dict['url'].append(\"https://api.p2pquake.net/v1/human-readable?limit={i}&offset={offset}&since_date={fecha_start}\")\n",
    "            except:dict['url'].append(None)\n",
    "            \n",
    "            try:dict['tsunami'].append(a['earthquake'][\"domesticTsunami\"])\n",
    "            except:dict['tsunami'].append(None)\n",
    "            dict['sig'].append(0)\n",
    "            \n",
    "            try:dict['title'].append(a['earthquake']['hypocenter']['name'])\n",
    "            except:dict['title'].append(None)\n",
    "            \n",
    "            try:dict['lng'].append(a['earthquake']['hypocenter']['longitude'])\n",
    "            except:dict['lng'].append(None)\n",
    "            \n",
    "            try:dict['lat'].append(a['earthquake']['hypocenter']['latitude'])\n",
    "            except:dict['lat'].append(None)\n",
    "            \n",
    "            try:dict['deepth'].append(a['earthquake']['hypocenter']['depth'])\n",
    "            except:dict['deepth'].append(None)\n",
    "        except: \n",
    "            pass\n",
    "    df=pd.DataFrame(dict)\n",
    "    fecha_min=df.time[-1:].values[0]\n",
    "    #print(fecha_min)\n",
    "    fecha_min=datetime.datetime.strptime(fecha_min, '%Y/%m/%d %H:%M:%S.%f')\n",
    "    fecha_min=int(datetime.datetime.strftime(fecha_min, '%Y%m%d'))\n",
    "    fecha_start=fecha_min\n",
    "    offset+=100\n",
    "    print(offset, fecha_min)\n",
    "    os.system(\"clear\")\n",
    "    \n",
    "print('fin')\n",
    "print(fecha_min)\n",
    "#borro duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "#filtro filas con vacíos\n",
    "filtros=[None,'']\n",
    "for i in filtros:\n",
    "    m1=df['mag'].values !=i\n",
    "    m2=df['title'].values !=i\n",
    "    m3=df['lng'].values !=i\n",
    "    m4=df['lat'].values !=i\n",
    "    m5=df['deepth'].values !=i\n",
    "    df=df[m1 & m2 & m3 & m4 & m5]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('japon_alert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.time[-1:].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acá: Sacar última información de japón, para alimentar db y crear alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-20T18:21:00+09:00\n",
      "2022-11-20 18:21:00\n",
      "https://www.jma.go.jp/bosai/quake/data/20221120182125_20221120181734_VXSE5k_1.json\n",
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>sig</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.7</td>\n",
       "      <td>Off the Coast of Kushiro</td>\n",
       "      <td>2022-11-20T18:21:00+09:00</td>\n",
       "      <td>https://www.jma.go.jp/bosai/quake/data/2022112...</td>\n",
       "      <td>This earthquake poses no tsunami risk.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>震源・震度情報</td>\n",
       "      <td>144.38</td>\n",
       "      <td>42.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mag                     place                       time  \\\n",
       "0  4.7  Off the Coast of Kushiro  2022-11-20T18:21:00+09:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.jma.go.jp/bosai/quake/data/2022112...   \n",
       "\n",
       "                                    tsunami  sig    title     lng    lat int  \n",
       "0  This earthquake poses no tsunami risk.\\n    0  震源・震度情報  144.38  42.99   3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df=pd.DataFrame(dict)\\ndf.head()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creo dicc vacio\n",
    "dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'int':[]}\n",
    "#pido los últimos sismos\n",
    "urlJMAList = \"https://www.jma.go.jp/bosai/quake/data/list.json\"\n",
    "#hago una lista de los últimos sismos\n",
    "responseJMAList = requests.get(urlJMAList)\n",
    "#la convierto a formato json\n",
    "jsonDataJMAList = responseJMAList.json()\n",
    "\n",
    "i=0\n",
    "#recorro la lista y solicito la info a la api\n",
    "#for elemento in range(len(jsonDataJMAList)):\n",
    "jsonDataJMALastDatetime = jsonDataJMAList[0][\"rdt\"]\n",
    "print(jsonDataJMALastDatetime)\n",
    "jsonDataJMALastDatetime = datetime.datetime.strptime(jsonDataJMALastDatetime, '%Y-%m-%dT%H:%M:%S+09:00')\n",
    "print(jsonDataJMALastDatetime)\n",
    "urlJMAEarthquakeData = \"https://www.jma.go.jp/bosai/quake/data/\" + jsonDataJMAList[i][\"json\"]\n",
    "print(urlJMAEarthquakeData)\n",
    "responseJMAEarthquakeData = requests.get(urlJMAEarthquakeData)\n",
    "print(responseJMAEarthquakeData)\n",
    "jsonDataJMAEarthquakeData = responseJMAEarthquakeData.json()\n",
    "#'mag'\n",
    "dict['mag'].append(jsonDataJMAEarthquakeData['Body']['Earthquake']['Magnitude'])\n",
    "#'place\n",
    "dict['place'].append(jsonDataJMAEarthquakeData['Body']['Earthquake']['Hypocenter']['Area']['enName'] )\n",
    "#time\n",
    "dict['time'].append(jsonDataJMAEarthquakeData['Head']['ReportDateTime'])\n",
    "#url\n",
    "dict['url'].append(urlJMAEarthquakeData)\n",
    "#tsunami\n",
    "dict['tsunami'].append(jsonDataJMAEarthquakeData['Body']['Comments']['ForecastComment']['enText'])\n",
    "#sig\n",
    "dict['sig'].append(0)\n",
    "#title\n",
    "dict['title'].append(jsonDataJMAEarthquakeData['Head']['Title'])\n",
    "\n",
    "#lat, long\n",
    "dict['lat'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lat'])\n",
    "\n",
    "dict['lng'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lon'])\n",
    "\n",
    "#intensidad:\n",
    "dict['int'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['MaxInt'])\n",
    "i+=1\n",
    "df=pd.DataFrame(dict)\n",
    "display(df.head())   \n",
    "'''df=pd.DataFrame(dict)\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlJMAEarthquakeData = \"https://api.p2pquake.net/v2/jma/quake\" + jsonDataJMAList[0][\"json\"]\n",
    "responseJMAEarthquakeData = requests.get(urlJMAEarthquakeData)\n",
    "print(responseJMAEarthquakeData)\n",
    "jsonDataJMAEarthquakeData = responseJMAEarthquakeData.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jma.go.jp/bosai/quake/data/20221119095803_20221119095504_VXSE5k_1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Carga de datos de USGS`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def cargar_paises(anio,mes,dia_fin):\n",
    "    \n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en tres tablas distintas\n",
    "    Sirve para respaldo\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['usa', 'japon', 'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "    error=[]\n",
    "    for pais in paises:\n",
    "        while anio > 1999:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name=pais,con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta)\n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "                error.append(traceback.print_exc())\n",
    "            # Decrementamos el mes\n",
    "            if mes == 1:\n",
    "                # Reducimos el año\n",
    "                anio -= 1\n",
    "                mes = 12\n",
    "            else:\n",
    "                mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "    errores['error']=error\n",
    "    display(errores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "#Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises(anio,mes,dia_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=limpieza_general_tabla(df)\n",
    "df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')]\n",
    "\n",
    "ERROR: raise AttributeError(\"Can only use .str accessor with string values!\")\n",
    "\n",
    "Error en la carga en la fecha: 2020-6-01 hasta 2020-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2019-7-01 hasta 2019-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-7-01 hasta 2018-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-6-01 hasta 2018-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2010-4-01 hasta 2010-4-30 del país: usa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\\ndf['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\\ndf['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\\ndf['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\\ndf['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\\ndf['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\n",
    "df['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\n",
    "df['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\n",
    "df['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\n",
    "df['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\n",
    "df['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_paises_sismos(anio,mes,dia_fin):\n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en una única tabla de hechos\n",
    "    Sirve para automatizar la carga incremental\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['japon', 'usa' ,'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "\n",
    "    while anio > 2000:\n",
    "        #recorre la lista de paises\n",
    "        for pais in paises:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Le decimos que es formato fecha\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "            \n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "\n",
    "\n",
    "        # Decrementamos el mes\n",
    "        if mes == 1:\n",
    "            # Reducimos el año\n",
    "            anio -= 1\n",
    "            mes = 12\n",
    "        else:\n",
    "            mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "\n",
    "    display(errores)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "# Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises_sismos(anio,mes,dia_fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852b27bee67d373d5d537f6ee9474570f8f7363f24457c4522e6a7b2aeb81c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
