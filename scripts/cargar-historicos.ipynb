{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "\n",
    "#ETL:\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#hacer los calendarios de iteración\n",
    "from dateutil.rrule import rrule, DAILY , MONTHLY\n",
    "\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import psycopg2\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings\n",
    "\n",
    "#Web scraping\n",
    "#Permiso de la web\n",
    "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'\n",
    "#desactivamos los request\n",
    "disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Creación de directorios</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#ff7700\">Crear una carpeta llamada \"scripts\" y colocar este notebook dentro de ella y ejecutar sus celdas</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha creado el directorio: e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup \n",
      "Se ha creado el directorio: e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/usa \n",
      "Se ha creado el directorio: e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/japon \n",
      "Se ha creado el directorio: e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/chile \n"
     ]
    }
   ],
   "source": [
    "#Se rutea los directorios y en caso de no existir se crean\n",
    "# Se define el nombre de la carpeta o directorio a crear\n",
    "path = pathlib.Path().absolute()\n",
    "directorio = f\"{path}/../backup\"\n",
    "try:\n",
    "    os.mkdir(directorio)\n",
    "except OSError:\n",
    "    print(\"La creación del directorio %s falló\" % directorio)\n",
    "else:\n",
    "    print(\"Se ha creado el directorio: %s \" % directorio)\n",
    "# Hago sub carpetas con el nombre de los paises\n",
    "directorios = ['usa','japon','chile']\n",
    "for direc in directorios:\n",
    "    ruta_backs = f\"{directorio}/{direc}\"\n",
    "    try:\n",
    "        os.mkdir(ruta_backs)\n",
    "    except OSError:\n",
    "        print(\"La creación del directorio %s falló\" % ruta_backs)\n",
    "    else:\n",
    "        print(\"Se ha creado el directorio: %s \" % ruta_backs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Acciones en DB</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a postgres mediante alquemy\n",
    "# De ser necesario, editar parámetros de conexión\n",
    "cone = create_engine('postgresql://airflow:airflow@localhost:5432/sismosdb', pool_size=50, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "# Creamos las tablas\n",
    "tabla_paises = 'DROP TABLE IF EXISTS PAIS CASCADE; CREATE TABLE PAIS (idpais SERIAL PRIMARY KEY NOT NULL ,pais text);'\n",
    "tabla_eeuu = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (idsismo SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_chile = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER, foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_japon = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,deepth float8, peligro smallint);'\n",
    "tabla_tsunamis = 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),altura_oleaje float8, place text, time timestamp, url smallint, mag float8, lng float8, lat float8, deepth float8);'\n",
    "tabla_volcanes = 'DROP TABLE IF EXISTS VOLCANES CASCADE; CREATE TABLE VOLCANES (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),nombre text, tipo text, elevacion float8, place text, ultima_erupcion text, lat float8, lng float8, url text);'\n",
    "\n",
    "\n",
    "# Editar parámetros de ser necesario\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='airflow',\n",
    "    password='airflow',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Ejecutamos la creación de las tablas\n",
    "cur.execute(tabla_paises)\n",
    "cur.execute(tabla_eeuu)\n",
    "cur.execute(tabla_chile)\n",
    "cur.execute(tabla_japon)\n",
    "cur.execute(tabla_hechos)\n",
    "cur.execute(tabla_tsunamis)\n",
    "cur.execute(tabla_volcanes)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Insertamos los países y los guardamos en una lista\n",
    "# para los siguientes procesos\n",
    "qry = \"\"\"INSERT INTO pais \\\n",
    "       (pais) \\\n",
    "       VALUES \\\n",
    "       ('USA'), ('Japón'), ('Chile');\"\"\"\n",
    "\n",
    "\n",
    "# Creamos las funciones almacenadas en la base de datos\n",
    "# para insertar en la tabla sismos cuando se inserten registros\n",
    "# en las tablas independientes de cada país\n",
    "func_usa = '''\n",
    "           create or replace function f_usa() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, deepth, peligro)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.deepth, new.peligro);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "func_japon = '''\n",
    "           create or replace function f_japon() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, deepth, peligro)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.deepth, new.peligro);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "func_chile = '''\n",
    "           create or replace function f_chile() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, deepth, peligro)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.deepth, new.peligro);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "cur.execute(func_usa)\n",
    "cur.execute(func_japon)\n",
    "cur.execute(func_chile)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Creamos los triggers que disparan las funciones de inserción\n",
    "trig_usa = '''\n",
    "           create trigger t_usa\n",
    "           after insert on usa\n",
    "           for each row\n",
    "           execute procedure f_usa()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "trig_japon = '''\n",
    "           create trigger t_japon\n",
    "           after insert on japon\n",
    "           for each row\n",
    "           execute procedure f_japon()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "trig_chile = '''\n",
    "           create trigger t_chile\n",
    "           after insert on chile\n",
    "           for each row\n",
    "           execute procedure f_chile()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "cur.execute(trig_usa)\n",
    "cur.execute(trig_japon)\n",
    "cur.execute(trig_chile)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(qry)\n",
    "conn.commit()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "# Cargamos los países con los ID asignados por la DB\n",
    "qry = 'SELECT * FROM pais;'\n",
    "df_paises = pd.read_sql(sql=qry, con=cone)\n",
    "\n",
    "usa = df_paises['idpais'][df_paises['pais'] == 'USA'].values[0]\n",
    "japon = df_paises['idpais'][df_paises['pais'] == 'Japón'].values[0]\n",
    "chile = df_paises['idpais'][df_paises['pais'] == 'Chile'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Funciones generales</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla(df):\n",
    "    '''\n",
    "    Función: limpieza de cadenas de string\n",
    "    Entrada: Data Frame a normalizar\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    #Acomodamos el indice\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "\n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c] = df[c].apply(lambda x:x.replace(elemento ,' ')if type(x) != float else x)                  \n",
    "\n",
    "        #sacamos los acentos\n",
    "        df[c] = df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c == 'place':\n",
    "            lista_palabras = [' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_palabras:\n",
    "                df[c] = df[c].apply(lambda x:x.replace(elemento ,' ')if type(x) != float else x)\n",
    "\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c] = df[c].apply(lambda x: 'sin dato' if type(x) == str and x == '' else x)\n",
    "        \n",
    "        # Eliminamos los registros con mag = 0 o deepth = 0\n",
    "        df = df[df['mag'] != 0]\n",
    "        df = df[df['deepth'] != 0]\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fechas():\n",
    "    '''\n",
    "    Retorna las fechas desde y hasta para los datos históricos de cada país\n",
    "    '''\n",
    "    # Definimos las fechas desde y hasta para la url\n",
    "    fecha_desde = dt.datetime(2022, 1, 1)\n",
    "    fecha_hasta = dt.datetime.today() - dt.timedelta(days=3)\n",
    "\n",
    "    return fecha_desde, fecha_hasta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Históricos USA</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def procesarDatos(url):\n",
    "    '''\n",
    "    Limpia y trasnforma los datos de la API\n",
    "    -> DataFrame\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    cant_reg = len(resp['features'])\n",
    "    for i in range(cant_reg):\n",
    "        mag = resp['features'][i]['properties']['mag']\n",
    "        place = resp['features'][i]['properties']['place']\n",
    "        time = resp['features'][i]['properties']['time']\n",
    "        url = resp['features'][i]['properties']['url']\n",
    "        tsunami = resp['features'][i]['properties']['tsunami']\n",
    "        title = resp['features'][i]['properties']['title']\n",
    "        lng = resp['features'][i]['geometry']['coordinates'][0]\n",
    "        lat = resp['features'][i]['geometry']['coordinates'][1]\n",
    "        deepth = resp['features'][i]['geometry']['coordinates'][2]\n",
    "        peligro = 1\n",
    "\n",
    "        # Vemos que no haya nulos para evitar errores al armar la cadena\n",
    "        if mag is None:\n",
    "            mag = 0\n",
    "        if place is None:\n",
    "            place = 'Sin dato'\n",
    "        if time is None:\n",
    "            time = '1900-01-01 00:00:00.000'\n",
    "        else:\n",
    "            time = dt.datetime.fromtimestamp(time//1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            time = dt.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "        if url is None:\n",
    "            url = 'Sin dato'\n",
    "        if tsunami is None:\n",
    "            tsunami = -1\n",
    "        if title is None:\n",
    "            title = 'Sin dato'\n",
    "        if lng is None:\n",
    "            lng = 0\n",
    "        if lat is None:\n",
    "            lat = 0\n",
    "        if deepth is None:\n",
    "            deepth = 0\n",
    "\n",
    "        # Cargamos el diccionario\n",
    "        datos['mag'].append(mag)\n",
    "        datos['place'].append(place)\n",
    "        datos['time'].append(time)\n",
    "        datos['url'].append(url)\n",
    "        datos['tsunami'].append(tsunami)\n",
    "        datos['title'].append(title)\n",
    "        datos['lng'].append(lng)\n",
    "        datos['lat'].append(lat)\n",
    "        datos['deepth'].append(deepth)\n",
    "\n",
    "    # Convertimos el diccionario a DataFrame\n",
    "    df_crudo = pd.DataFrame(datos)\n",
    "\n",
    "    # Obtenemos los 5 vlores más altos en magnitud\n",
    "    df_crudo = df_crudo.nlargest(5, ['mag'])\n",
    "\n",
    "    # Limpiamos los datos\n",
    "    df_crudo = limpieza_general_tabla(df_crudo)\n",
    "\n",
    "    # Agregamos la columna idpais y peligro por defecto -1 (todavía no esta hecha la predicción)\n",
    "    df_crudo['idpais'] = usa    \n",
    "    df_crudo['peligro'] = -1\n",
    "\n",
    "    return df_crudo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def consultarAPIUsa():\n",
    "    '''\n",
    "    Consulta la API de USA\n",
    "    -> DataFrame\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    for fecha in rrule(DAILY, dtstart=fecha_d, until=fecha_h):\n",
    "        fecha = dt.datetime.strftime(fecha, '%Y-%m-%d')\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "\n",
    "    for i in range(len(lista_fechas) - 1):\n",
    "        fecha_desde = lista_fechas[i]\n",
    "        fecha_hasta = lista_fechas[i + 1]\n",
    "   \n",
    "        # armamos la url\n",
    "        url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=23.300000&maxlatitude=69.400000&minlongitude=-160.000000&maxlongitude=-69.500000&jsonerror=true'\n",
    "\n",
    "        # Obtenemos el DataFrame ya procesado\n",
    "        df_procesado = procesarDatos(url)\n",
    "\n",
    "        if not df_procesado.empty:\n",
    "            df_procesado.to_csv(F'../backup/usa/{fecha_desde}.csv', index=False)\n",
    "            df_procesado.to_sql(name='usa', con=cone, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos la carga de USA\n",
    "consultarAPIUsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Históricos Chile</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_chile(dt):\n",
    "    year = dt.strftime(\"%Y\")\n",
    "    month = dt.strftime(\"%m\")\n",
    "    day = dt.strftime('%Y%m%d')\n",
    "\n",
    "    url = f\"https://www.sismologia.cl/sismicidad/catalogo/{year}/{month}/{day}.html\"\n",
    "    urlx = requests.get(url)\n",
    "    soup = BeautifulSoup(urlx.content,\"html.parser\")\n",
    "    if soup is not None:\n",
    "        rows = soup.find(\"table\", attrs={\"class\":\"sismologia detalle\"}).find_all(\"tr\")\n",
    "        return  url, rows\n",
    "    else:\n",
    "        return url, rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def datos_chile(url, rows):\n",
    "    #creo dicc vacio\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    for index, i in enumerate(rows):\n",
    "        if index == 0:\n",
    "            pass\n",
    "        else:\n",
    "            datos['mag'].append(float(rows[index].find_all(\"td\")[-1].get_text()[:-3]))\n",
    "            datos['place'].append(rows[index].find_all(\"td\")[0].get_text()[19:])\n",
    "\n",
    "            time = dt.datetime.strptime(rows[index].find_all(\"td\")[1].get_text(),'%Y-%m-%d %H:%M:%S')\n",
    "            time = str(time)\n",
    "            time = time[2 : -2]\n",
    "            datos['time'].append(time)\n",
    "\n",
    "\n",
    "            datos['url'].append(url)\n",
    "            datos['tsunami'].append(-1)\n",
    "            datos['title'].append('Sin dato')\n",
    "\n",
    "            datos['lat'].append(float(rows[index].find_all(\"td\")[2].get_text()[:7]))\n",
    "            datos['lng'].append(float(rows[index].find_all(\"td\")[2].get_text()[-7:]))\n",
    "            datos['deepth'].append(float(rows[index].find_all(\"td\")[3].get_text().split()[0]))\n",
    "\n",
    "    for elemento in datos:\n",
    "        len(datos[elemento])\n",
    "\n",
    "    df_chile = pd.DataFrame(datos)\n",
    "\n",
    "    return df_chile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def carga_historica_chile():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    for fecha in rrule(DAILY, dtstart=fecha_d, until=fecha_h):\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "\n",
    "    for i in range(len(lista_fechas)):\n",
    "        fecha = lista_fechas[i]\n",
    "\n",
    "        url, rows = get_url_chile(fecha)    \n",
    "\n",
    "        df = datos_chile(url, rows)\n",
    "        df = limpieza_general_tabla(df)\n",
    "\n",
    "        df = df[(df['lat'] > -50.0) & (df['lat'] < -18.0) & (df['lng'] > -78.0) & (df['lng'] < -60.0)]\n",
    "        df.insert(loc = 0, column = 'idpais', value = chile)                    \n",
    "        df['peligro'] = -1\n",
    "\n",
    "        if not df.empty:\n",
    "            fecha_str = dt.datetime.strftime(fecha, '%Y-%m-%d')\n",
    "            df.to_csv(F'../backup/chile/{fecha_str}.csv', index=False)\n",
    "            df.to_sql(name='chile', con=cone, if_exists='append', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la carga de Chile\n",
    "carga_historica_chile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff7700\">Históricos Japón</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(date):\n",
    "    '''\n",
    "    Función:Generar la url custom para la fecha y los datos para esa fecha\n",
    "    Entrada: Toma como ingesta la fecha en formato %YYYY%mm%dd, ej 20221102\n",
    "    Salida: Devuelve la url para esa fecha y las filas de la tabla obtenida para esa fecha\n",
    "    '''\n",
    "    # Agregamos \"0\" si el número de mes es un solo dígito\n",
    "    mes = str(date.month)\n",
    "    if len(mes) == 1:\n",
    "        mes = '0' + mes\n",
    "\n",
    "    #url\n",
    "    url = f'https://www.hinet.bosai.go.jp/AQUA/aqua_catalogue.php?y={date.year}&m={mes}&LANG=en'\n",
    "\n",
    "    page = requests.get(url)\n",
    "    #leemos el html\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "    #Pasamos a filas\n",
    "    rows1 = soup.find(\"table\", attrs={\"class\":\"base\"}).find_all(\"tr\")\n",
    "    \n",
    "    return url, rows1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def historicos_japon(url, rows1):\n",
    "    '''\n",
    "    Función:\n",
    "    Entrada: Toma como ingesta la url para una fecha y las filas\n",
    "    Salida: Devuelve un df con todos los sismos de japón para todo el mes de esa fecha\n",
    "    '''\n",
    "    #creo dicc vacio\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    for index in range(5,len(rows1) - 1):\n",
    "        datos['mag'].append(float(rows1[index].find_all(\"td\")[5].get_text()))\n",
    "        datos['place'].append(rows1[index].find_all(\"td\")[1].get_text())\n",
    "\n",
    "        time = dt.datetime.strptime(rows1[index].find_all(\"td\")[0].get_text(),'%Y-%m-%d %H:%M:%S')\n",
    "        time = str(time)\n",
    "        time = time[2 : -2]\n",
    "        datos['time'].append(time)\n",
    "\n",
    "        datos['url'].append(url)\n",
    "        datos['tsunami'].append(-1)\n",
    "        datos['title'].append('Sin dato')\n",
    "\n",
    "        datos['lng'].append(float(rows1[index].find_all(\"td\")[3].get_text()[:-1]))\n",
    "        datos['lat'].append(float(rows1[index].find_all(\"td\")[2].get_text()[:-1]))\n",
    "        datos['deepth'].append(float(rows1[index].find_all(\"td\")[4].get_text().split('km')[0]))\n",
    "\n",
    "    for elemento in datos:\n",
    "        len(datos[elemento])\n",
    "\n",
    "    df_japon = pd.DataFrame(datos)\n",
    "\n",
    "    return df_japon\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def carga_historica_japon():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    # lista de días\n",
    "    lista_dias = list(range(1, 32))\n",
    "    for i in range(len(lista_dias)):\n",
    "        lista_dias[i] = str(lista_dias[i])\n",
    "        if len(lista_dias[i]) == 1:\n",
    "            lista_dias[i] = '0' + lista_dias[i]\n",
    "\n",
    "\n",
    "    for fecha in rrule(MONTHLY, dtstart=fecha_d, until=fecha_h):\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "    for i in range(len(lista_fechas)):\n",
    "        fecha = lista_fechas[i]\n",
    "        url, rows1 = get_url(fecha)\n",
    "\n",
    "        df = historicos_japon(url, rows1)\n",
    "\n",
    "        df = limpieza_general_tabla(df)\n",
    "\n",
    "        df.insert(loc = 0, column = 'idpais', value = japon)                    \n",
    "        df['peligro'] = -1\n",
    "        \n",
    "\n",
    "        # Eliminamos los que no corresponden al día solicitado, dado que\n",
    "        # la API de Japón devuelve todo el mes aunque se especifique el día\n",
    "        for dia in lista_dias:\n",
    "            #if not df_cop.empty:\n",
    "            df_mask = df['time'].apply(lambda x: x[4:6] == dia)\n",
    "            df_cop = df[df_mask]\n",
    "\n",
    "\n",
    "            if not df_cop.empty:\n",
    "                fecha_str = dt.datetime.strftime(fecha, '%Y-%m-') + dia\n",
    "                df_cop.to_csv(f'../backup/japon/{fecha_str}.csv')\n",
    "                df_cop.to_sql(name='japon', con=cone, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la carga de Japón\n",
    "carga_historica_japon()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a06e0cf41d9bd1fb106e7e83326df00b44cf746455b39376adea504742b82eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
