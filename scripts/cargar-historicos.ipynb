{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Índice`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importación de librerías \n",
    "* Directorio backup\n",
    "* Creación de tablas\n",
    "* Funciones generales \n",
    "* Fecha de inicio y fin de la carga de datos\n",
    "* Tabla paises\n",
    "* Históricos USA\n",
    "* Históricos Chile\n",
    "* Históricos Japón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Importación de librerías`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "\n",
    "#ETL:\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#hacer los calendarios de iteración\n",
    "from dateutil.rrule import rrule, DAILY , MONTHLY\n",
    "\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "import datetime as dt\n",
    "import psycopg2\n",
    "\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "from urllib3 import disable_warnings\n",
    "\n",
    "#Web scraping\n",
    "#Permiso de la web\n",
    "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = 'ALL:@SECLEVEL=1'\n",
    "#desactivamos los request\n",
    "disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Creación de directorios`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La creación del directorio e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup falló\n",
      "La creación del directorio e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/usa falló\n",
      "La creación del directorio e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/japon falló\n",
      "La creación del directorio e:\\HENRY\\DataScience\\LABS\\ProyectoFinal\\script/../backup/chile falló\n"
     ]
    }
   ],
   "source": [
    "#Se rutea los directorios y en caso de no existir se crean\n",
    "# Se define el nombre de la carpeta o directorio a crear\n",
    "path = pathlib.Path().absolute()\n",
    "directorio = f\"{path}/../backup\"\n",
    "try:\n",
    "    os.mkdir(directorio)\n",
    "except OSError:\n",
    "    print(\"La creación del directorio %s falló\" % directorio)\n",
    "else:\n",
    "    print(\"Se ha creado el directorio: %s \" % directorio)\n",
    "# Hago sub carpetas con el nombre de los paises\n",
    "directorios = ['usa','japon','chile']\n",
    "for direc in directorios:\n",
    "    ruta_backs = f\"{directorio}/{direc}\"\n",
    "    try:\n",
    "        os.mkdir(ruta_backs)\n",
    "    except OSError:\n",
    "        print(\"La creación del directorio %s falló\" % ruta_backs)\n",
    "    else:\n",
    "        print(\"Se ha creado el directorio: %s \" % ruta_backs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Creación de tablas`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión a postgres mediante alquemy\n",
    "# De ser necesario, editar parámetros de conexión\n",
    "cone = create_engine('postgresql://airflow:airflow@localhost:5432/sismosdb', pool_size=50, max_overflow=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "# Creamos las tablas\n",
    "tabla_paises = 'DROP TABLE IF EXISTS PAIS CASCADE; CREATE TABLE PAIS (idpais SERIAL PRIMARY KEY NOT NULL ,pais text);'\n",
    "tabla_eeuu = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (idsismo SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,depth float8, peligro smallint,year INTEGER,month INTEGER,day INTEGER);'\n",
    "tabla_chile = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER, foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,depth float8, peligro smallint,year INTEGER,month INTEGER,day INTEGER);'\n",
    "tabla_japon = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,depth float8, peligro smallint,year INTEGER,month INTEGER,day INTEGER);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (idsismo SERIAL PRIMARY KEY NOT NULL,idpais INTEGER,foreign key (idpais) references PAIS(idpais),mag float8, place text,time timestamp,url text,tsunami smallint, title text,lng float8, lat float8,depth float8, peligro smallint,year INTEGER,month INTEGER,day INTEGER);'\n",
    "tabla_tsunamis = 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),altura_oleaje float8, place text, time timestamp, url text, mag float8, lng float8, lat float8, depth float8,year INTEGER,month INTEGER,day INTEGER);'\n",
    "tabla_volcanes = 'DROP TABLE IF EXISTS VOLCANES CASCADE; CREATE TABLE VOLCANES (id SERIAL PRIMARY KEY NOT NULL ,idpais INTEGER,foreign key (idpais) references PAIS(idpais),nombre text, tipo text, elevacion float8, place text, ultima_erupcion text, lat float8, lng float8, url text);'\n",
    "\n",
    "\n",
    "# Editar parámetros de ser necesario\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='airflow',\n",
    "    password='airflow',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Ejecutamos la creación de las tablas\n",
    "cur.execute(tabla_paises)\n",
    "cur.execute(tabla_eeuu)\n",
    "cur.execute(tabla_chile)\n",
    "cur.execute(tabla_japon)\n",
    "cur.execute(tabla_hechos)\n",
    "cur.execute(tabla_tsunamis)\n",
    "cur.execute(tabla_volcanes)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Insertamos los países y los guardamos en una lista\n",
    "# para los siguientes procesos\n",
    "qry = \"\"\"INSERT INTO pais \\\n",
    "       (pais) \\\n",
    "       VALUES \\\n",
    "       ('USA'), ('Japón'), ('Chile');\"\"\"\n",
    "\n",
    "\n",
    "# Creamos las funciones almacenadas en la base de datos\n",
    "# para insertar en la tabla sismos cuando se inserten registros\n",
    "# en las tablas independientes de cada país\n",
    "func_usa = '''\n",
    "           create or replace function f_usa() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, depth, peligro, year,month, day)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.depth, new.peligro, new.year,new.month,new.day);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "func_japon = '''\n",
    "           create or replace function f_japon() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, depth, peligro, year,month, day)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.depth, new.peligro, new.year,new.month,new.day);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "func_chile = '''\n",
    "           create or replace function f_chile() \n",
    "                  returns trigger \n",
    "                  language plpgsql\n",
    "                  as\n",
    "           $$\n",
    "           declare\n",
    "           begin\n",
    "                  insert into sismos\n",
    "           (idpais, mag, place, time, url, tsunami, title, lng, lat, depth, peligro, year,month, day)\n",
    "           values\n",
    "           (new.idpais, new.mag, new.place, new.time, new.url, new.tsunami, new.title, new.lng, new.lat, new.depth, new.peligro, new.year,new.month,new.day);\n",
    "           return null;\n",
    "           end;\n",
    "           $$\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "cur.execute(func_usa)\n",
    "cur.execute(func_japon)\n",
    "cur.execute(func_chile)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "# Creamos los triggers que disparan las funciones de inserción\n",
    "trig_usa = '''\n",
    "           create trigger t_usa\n",
    "           after insert on usa\n",
    "           for each row\n",
    "           execute procedure f_usa()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "trig_japon = '''\n",
    "           create trigger t_japon\n",
    "           after insert on japon\n",
    "           for each row\n",
    "           execute procedure f_japon()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "trig_chile = '''\n",
    "           create trigger t_chile\n",
    "           after insert on chile\n",
    "           for each row\n",
    "           execute procedure f_chile()\n",
    "           ;\n",
    "           '''\n",
    "\n",
    "cur.execute(trig_usa)\n",
    "cur.execute(trig_japon)\n",
    "cur.execute(trig_chile)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(qry)\n",
    "conn.commit()\n",
    "\n",
    "# Cargamos los países con los ID asignados por la DB\n",
    "qry = 'SELECT * FROM pais;'\n",
    "df_paises = pd.read_sql(sql=qry, con=cone)\n",
    "\n",
    "usa = df_paises['idpais'][df_paises['pais'] == 'USA'].values[0]\n",
    "japon = df_paises['idpais'][df_paises['pais'] == 'Japón'].values[0]\n",
    "chile = df_paises['idpais'][df_paises['pais'] == 'Chile'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editar parámetros de ser necesario\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='postgres',\n",
    "    password='postgres',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "# Cargamos los países con los ID asignados por la DB\n",
    "#qry = 'SELECT * FROM chile;'\n",
    "#df_paises = pd.read_sql(sql=qry, con=conn)\n",
    "#df_paises.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Funciones generales`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla(df):\n",
    "    '''\n",
    "    Función: limpieza de cadenas de string\n",
    "    Entrada: Data Frame a normalizar\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    #Acomodamos el indice\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "\n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c] = df[c].apply(lambda x:x.replace(elemento ,' ')if type(x) != float else x)                  \n",
    "\n",
    "        #sacamos los acentos\n",
    "        df[c] = df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c == 'place':\n",
    "            lista_palabras = [' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_palabras:\n",
    "                df[c] = df[c].apply(lambda x:x.replace(elemento ,' ')if type(x) != float else x)\n",
    "\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c] = df[c].apply(lambda x: 'sin dato' if type(x) == str and x == '' else x)\n",
    "       \n",
    "        # Eliminamos los registros con mag = 0 o deepth = 0\n",
    "        df = df[df['mag'] != 0]\n",
    "        df = df[df['depth'] != 0]\n",
    "        \n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Definimos fecha de inicio y fin de la carga de datos`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fechas():\n",
    "    '''\n",
    "    Retorna las fechas desde y hasta para los datos históricos de cada país\n",
    "    '''\n",
    "    # Definimos las fechas desde y hasta para la url\n",
    "    fecha_desde = dt.datetime(2020, 1, 1)\n",
    "    fecha_hasta = dt.datetime.today() - dt.timedelta(days=3)\n",
    "\n",
    "    return fecha_desde, fecha_hasta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Históricos USA`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def procesarDatos(url):\n",
    "    '''\n",
    "    Limpia y trasnforma los datos de la API\n",
    "    -> DataFrame\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'depth':[]}\n",
    "\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    cant_reg = len(resp['features'])\n",
    "    for i in range(cant_reg):\n",
    "        mag = resp['features'][i]['properties']['mag']\n",
    "        place = resp['features'][i]['properties']['place']\n",
    "        time = resp['features'][i]['properties']['time']\n",
    "        url = resp['features'][i]['properties']['url']\n",
    "        tsunami = resp['features'][i]['properties']['tsunami']\n",
    "        title = resp['features'][i]['properties']['title']\n",
    "        lng = resp['features'][i]['geometry']['coordinates'][0]\n",
    "        lat = resp['features'][i]['geometry']['coordinates'][1]\n",
    "        depth = resp['features'][i]['geometry']['coordinates'][2]\n",
    "\n",
    "        # Vemos que no haya nulos para evitar errores al armar la cadena\n",
    "        if mag is None:\n",
    "            mag = 0\n",
    "        if place is None:\n",
    "            place = 'Sin dato'\n",
    "        if time is None:\n",
    "            time = dt.datetime(1900,1,1)\n",
    "        else:\n",
    "            time = dt.datetime.fromtimestamp(time//1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            time = dt.datetime.strptime(time, '%Y-%m-%d %H:%M:%S')\n",
    "        if url is None:\n",
    "            url = 'Sin dato'\n",
    "        if tsunami is None:\n",
    "            tsunami = -1\n",
    "        if title is None:\n",
    "            title = 'Sin dato'\n",
    "        if lng is None:\n",
    "            lng = 0\n",
    "        if lat is None:\n",
    "            lat = 0\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "\n",
    "        # Cargamos el diccionario\n",
    "        datos['mag'].append(mag)\n",
    "        datos['place'].append(place)\n",
    "        datos['time'].append(time)\n",
    "        datos['url'].append(url)\n",
    "        datos['tsunami'].append(tsunami)\n",
    "        datos['title'].append(title)\n",
    "        datos['lng'].append(lng)\n",
    "        datos['lat'].append(lat)\n",
    "        datos['depth'].append(depth)\n",
    "\n",
    "    # Convertimos el diccionario a DataFrame\n",
    "    df_crudo = pd.DataFrame(datos)\n",
    "\n",
    "    # Obtenemos los 5 vlores más altos en magnitud\n",
    "    #df_crudo = df_crudo.nlargest(5, ['mag'])\n",
    "\n",
    "    # Limpiamos los datos\n",
    "    df_crudo = limpieza_general_tabla(df_crudo)\n",
    "\n",
    "    # Agregamos la columna idpais y peligro por defecto -1 (todavía no esta hecha la predicción)\n",
    "    df_crudo['idpais'] = usa    \n",
    "    df_crudo['peligro'] = -1\n",
    "    df_crudo['year']=df_crudo.time.apply(lambda x: x.year)\n",
    "    df_crudo['month']=df_crudo.time.apply(lambda x: x.month)\n",
    "    df_crudo['day']=df_crudo.time.apply(lambda x: x.day)\n",
    "    df_crudo.place= df_crudo.place.replace(to_replace=r'ca$', value='canada', regex=True)\n",
    "\n",
    "    return df_crudo\n",
    "\n",
    "\n",
    "\n",
    "def consultarAPIUsa():\n",
    "    '''\n",
    "    Consulta la API de USA\n",
    "    -> DataFrame\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    for fecha in rrule(DAILY, dtstart=fecha_d, until=fecha_h):\n",
    "        fecha = dt.datetime.strftime(fecha, '%Y-%m-%d')\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "\n",
    "    for i in range(len(lista_fechas) - 1):\n",
    "        fecha_desde = lista_fechas[i]\n",
    "        fecha_hasta = lista_fechas[i + 1]\n",
    "   \n",
    "        # armamos la url\n",
    "        url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=23.300000&maxlatitude=69.400000&minlongitude=-160.000000&maxlongitude=-69.500000&jsonerror=true'\n",
    "\n",
    "        # Obtenemos el DataFrame ya procesado\n",
    "        df_procesado = procesarDatos(url)\n",
    "\n",
    "        # Eliminamos los puntos que no correspondan a USA\n",
    "        mask_1 = (df_procesado['lat'] > 20.0) & \\\n",
    "                 (df_procesado['lat'] < 70.0) & \\\n",
    "                 (df_procesado['lng'] > -155.0) & \\\n",
    "                 (df_procesado['lng'] < -65.0)\n",
    "\n",
    "        df_procesado = df_procesado[mask_1]                 \n",
    "\n",
    "\n",
    "        if not df_procesado.empty:\n",
    "            #display(df_procesado)\n",
    "            #print(df_procesado.dtypes)\n",
    "            df_procesado.to_csv(F'../backup/usa/{fecha_desde}.csv', index=False)\n",
    "            df_procesado.to_sql(name='usa', con=cone, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos la carga de USA\n",
    "consultarAPIUsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Históricos Chile`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_chile(dt):\n",
    "    year = dt.strftime(\"%Y\")\n",
    "    month = dt.strftime(\"%m\")\n",
    "    day = dt.strftime('%Y%m%d')\n",
    "\n",
    "    url = f\"http://www.sismologia.cl/sismicidad/catalogo/{year}/{month}/{day}.html\"\n",
    "    urlx = requests.get(url)\n",
    "    soup = BeautifulSoup(urlx.content,\"html.parser\")\n",
    "    if soup is not None:\n",
    "        rows = soup.find(\"table\", attrs={\"class\":\"sismologia detalle\"}).find_all(\"tr\")\n",
    "        return  url, rows\n",
    "    else:\n",
    "        return url, rows\n",
    "\n",
    "def datos_chile(url, rows):\n",
    "    #creo dicc vacio\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'depth':[]}\n",
    "    for index, i in enumerate(rows):\n",
    "        if index == 0:\n",
    "            pass\n",
    "        else:\n",
    "            datos['mag'].append(float(rows[index].find_all(\"td\")[-1].get_text()[:-3]))\n",
    "            datos['place'].append(rows[index].find_all(\"td\")[0].get_text()[19:])\n",
    "            datos['time'].append(dt.datetime.strptime(rows[index].find_all(\"td\")[1].get_text(),'%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "\n",
    "            datos['url'].append(url)\n",
    "            datos['tsunami'].append(-1)\n",
    "            datos['title'].append('Sin dato')\n",
    "\n",
    "            datos['lat'].append(float(rows[index].find_all(\"td\")[2].get_text()[:7]))\n",
    "            datos['lng'].append(float(rows[index].find_all(\"td\")[2].get_text()[-7:]))\n",
    "            datos['depth'].append(float(rows[index].find_all(\"td\")[3].get_text().split()[0]))\n",
    "\n",
    "    for elemento in datos:\n",
    "        len(datos[elemento])\n",
    "\n",
    "    df_chile = pd.DataFrame(datos)\n",
    "\n",
    "    return df_chile\n",
    "\n",
    "def carga_historica_chile():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    for fecha in rrule(DAILY, dtstart=fecha_d, until=fecha_h):\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "    for fecha in rrule(DAILY, dtstart=fecha_d, until=fecha_h):\n",
    "    #for i in range(len(lista_fechas)):\n",
    "        #fecha = lista_fechas[i]\n",
    "\n",
    "        url, rows = get_url_chile(fecha)    \n",
    "\n",
    "        df = datos_chile(url, rows)\n",
    "        df = limpieza_general_tabla(df)\n",
    "\n",
    "        # Eliminamos los puntos que no correspondan a Chile\n",
    "        df = df[(df['lat'] > -50.0)  &\n",
    "                (df['lat'] < -18.0) & \n",
    "                (df['lng'] > -78.0) & \n",
    "                (df['lng'] < -60.0)]\n",
    "\n",
    "        df = df[(df['lat'] > -50.0) & (df['lat'] < -18.0) & (df['lng'] > -78.0) & (df['lng'] < -60.0)]\n",
    "        df.insert(loc = 0, column = 'idpais', value = chile)                    \n",
    "        df['peligro'] = -1\n",
    "        df['year']=df.time.apply(lambda x: x.year)\n",
    "        df['month']=df.time.apply(lambda x: x.month)\n",
    "        df['day']=df.time.apply(lambda x: x.day)\n",
    "            \n",
    "\n",
    "        if not df.empty:\n",
    "            fecha_str = dt.datetime.strftime(fecha, '%Y-%m-%d')\n",
    "            df.to_csv(F'../backup/chile/{fecha_str}.csv', index=False)\n",
    "            df.to_sql(name='chile', con=cone, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la carga de Chile\n",
    "carga_historica_chile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Históricos Japón`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(date):\n",
    "    '''\n",
    "    Función:Generar la url custom para la fecha y los datos para esa fecha\n",
    "    Entrada: Toma como ingesta la fecha en formato %YYYY%mm%dd, ej 20221102\n",
    "    Salida: Devuelve la url para esa fecha y las filas de la tabla obtenida para esa fecha\n",
    "    '''\n",
    "    # Agregamos \"0\" si el número de mes es un solo dígito\n",
    "    mes = str(date.month)\n",
    "    if len(mes) == 1:\n",
    "        mes = '0' + mes\n",
    "\n",
    "    #url\n",
    "    url = f'https://www.hinet.bosai.go.jp/AQUA/aqua_catalogue.php?y={date.year}&m={mes}&LANG=en'\n",
    "\n",
    "    page = requests.get(url)\n",
    "    #leemos el html\n",
    "    soup = BeautifulSoup(page.content,\"html.parser\")\n",
    "    #Pasamos a filas\n",
    "    rows1 = soup.find(\"table\", attrs={\"class\":\"base\"}).find_all(\"tr\")\n",
    "    \n",
    "    return url, rows1\n",
    "\n",
    "\n",
    "def historicos_japon(url, rows1):\n",
    "    '''\n",
    "    Función:\n",
    "    Entrada: Toma como ingesta la url para una fecha y las filas\n",
    "    Salida: Devuelve un df con todos los sismos de japón para todo el mes de esa fecha\n",
    "    '''\n",
    "    #creo dicc vacio\n",
    "    datos = {'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'depth':[], 'year':[], 'month':[], 'day':[]}\n",
    "    for index in range(5,len(rows1) - 1):\n",
    "        datos['mag'].append(float(rows1[index].find_all(\"td\")[5].get_text()))\n",
    "        datos['place'].append(rows1[index].find_all(\"td\")[1].get_text())\n",
    "\n",
    "        time = dt.datetime.strptime(rows1[index].find_all(\"td\")[0].get_text(),'%Y-%m-%d %H:%M:%S')\n",
    "        datos['year'].append(time.year)\n",
    "        datos['month'].append(time.month)\n",
    "        datos['day'].append(time.day)\n",
    "        time = str(time)\n",
    "        time = time[2 : -2]\n",
    "        datos['time'].append(time)\n",
    "\n",
    "        datos['url'].append(url)\n",
    "        datos['tsunami'].append(-1)\n",
    "        datos['title'].append('Sin dato')\n",
    "\n",
    "        datos['lng'].append(float(rows1[index].find_all(\"td\")[3].get_text()[:-1]))\n",
    "        datos['lat'].append(float(rows1[index].find_all(\"td\")[2].get_text()[:-1]))\n",
    "        datos['depth'].append(float(rows1[index].find_all(\"td\")[4].get_text().split('km')[0]))\n",
    "\n",
    "    for elemento in datos:\n",
    "        len(datos[elemento])\n",
    "\n",
    "    df_japon = pd.DataFrame(datos)\n",
    "\n",
    "    return df_japon\n",
    "\n",
    "\n",
    "def carga_historica_japon():\n",
    "    '''\n",
    "    Función cargar datos de chile en postgres y hacer csv de backup\n",
    "    Entrada: Fecha de ingreso en formato date, fecha de fin de la carga en formato date\n",
    "    Salida: Notificación de finalizada la carga\n",
    "    '''\n",
    "    fecha_d, fecha_h = fechas()\n",
    "    lista_fechas = []\n",
    "\n",
    "    # lista de días\n",
    "    lista_dias = list(range(1, 32))\n",
    "    for i in range(len(lista_dias)):\n",
    "        lista_dias[i] = str(lista_dias[i])\n",
    "        if len(lista_dias[i]) == 1:\n",
    "            lista_dias[i] = '0' + lista_dias[i]\n",
    "\n",
    "\n",
    "    for fecha in rrule(MONTHLY, dtstart=fecha_d, until=fecha_h):\n",
    "        lista_fechas.append(fecha)\n",
    "\n",
    "    for i in range(len(lista_fechas)):\n",
    "        fecha = lista_fechas[i]\n",
    "        url, rows1 = get_url(fecha)\n",
    "\n",
    "        df = historicos_japon(url, rows1)\n",
    "\n",
    "        df = limpieza_general_tabla(df)\n",
    "\n",
    "        # Eliminamos los puntos que no correspondan a Japón\n",
    "        mask_1 = (df['lat'] > 25.0) & \\\n",
    "                 (df['lat'] < 47.0) & \\\n",
    "                 (df['lng'] > 130.0) & \\\n",
    "                 (df['lng'] < 150.0)\n",
    "\n",
    "        df = df[mask_1]\n",
    "\n",
    "        df.insert(loc = 0, column = 'idpais', value = japon)                    \n",
    "        df['peligro'] = -1\n",
    "        \n",
    "\n",
    "        # Eliminamos los que no corresponden al día solicitado, dado que\n",
    "        # la API de Japón devuelve todo el mes aunque se especifique el día\n",
    "        for dia in lista_dias:\n",
    "            df_mask = df['time'].apply(lambda x: x[4:6] == dia)\n",
    "            df_cop = df[df_mask]\n",
    "\n",
    "\n",
    "            if not df_cop.empty:\n",
    "                fecha_str = dt.datetime.strftime(fecha, '%Y-%m-') + dia\n",
    "                df_cop.to_csv(f'../backup/japon/{fecha_str}.csv')\n",
    "                df_cop.to_sql(name='japon', con=cone, if_exists='append', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la carga de Japón\n",
    "carga_historica_japon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`NOAAA`***\n",
    "Documentation:\n",
    "* https://www.ngdc.noaa.gov/hazel/view/swagger#/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={'usa':1 , 'japon':2,'chile':3}\n",
    "\n",
    "def paises(pais):\n",
    "    '''\n",
    "    Función:Generar claves id pais\n",
    "    Entrada: pais a dar/crear clave\n",
    "    Salida:clave idpais\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    global dic\n",
    "    if pais in dic:\n",
    "        #df.to_sql(name='pais',con=cone, if_exists='replace', index=False)\n",
    "        return(dic[pais])\n",
    "    else:\n",
    "        dic[pais]=len(dic)+1\n",
    "        dic_pais={'pais':[pais]}\n",
    "        df=pd.DataFrame(dic_pais)\n",
    "        df.to_sql(name='pais',con=cone, if_exists='append', index=False)\n",
    "        return(dic[pais])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Tsunamis`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsunami_Noaa():\n",
    "    ''' \n",
    "    Función:\n",
    "    Entrada:\n",
    "    Salida:    \n",
    "    '''\n",
    "    countries = ['chile', 'japan', 'usa']\n",
    "    for country in countries:\n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?country={country.upper()}&maxYear=2022&minYear=2000'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Creamos la columna con la fecha del tsunami\n",
    "        df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']],format='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'depth', 'latitude':'lat', 'longitude':'lng',\n",
    "                   'locationName':'place', 'maxWaterHeight':'altura_oleaje'}, inplace=True)\n",
    "\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        if country=='japan':\n",
    "            country='japon'\n",
    "        df.insert(loc = 0, column = 'idpais', value =paises(country))\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "        \n",
    "        \n",
    "        df['altura_oleaje']  = df.altura_oleaje.astype('float64')\n",
    "        df.mag=df.mag.astype('float64')\n",
    "        df.lat=df.lng.astype('float64')\n",
    "        df.lat=df.lat.astype('float64')\n",
    "        df.depth=df.depth.astype('float64')\n",
    "\n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','altura_oleaje','place', 'time', 'url', 'mag', 'lng','lat', 'depth' ]]\n",
    "    \n",
    "        \n",
    "        df['year']=df['time'].apply(lambda x: x.year)\n",
    "        df['month']=df['time'].apply(lambda x: x.month)\n",
    "        df['day']=df['time'].apply(lambda x: x.day)\n",
    "        #Info para terminar de cargar la tabla\n",
    "        # #Paso la tabla a Potgres        \n",
    "        df.to_sql(name='tsunamis',con=cone, if_exists='append', index=False)\n",
    "        print(f'La carga se ha hecho con exito!{country}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carga se ha hecho con exito!chile\n",
      "La carga se ha hecho con exito!japon\n",
      "La carga se ha hecho con exito!usa\n"
     ]
    }
   ],
   "source": [
    "df1=tsunami_Noaa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Sismos`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Los sismos ontenidos de la API de Noaa cumplen una o varias de las siguientes condiciones:***\n",
    "* Generaron un daño moderado (arpoximadamente 1 millon USD o más).\n",
    "* Produjeron 10 o más muertes.\n",
    "* Magnitud mayor a 7.5.\n",
    "* Intendidad del sismo en escala de Mercalli mayor o igual a X.\n",
    "* Produjo un tsunami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sismos_Noaa():\n",
    "    ''' \n",
    "    Función:\n",
    "    Entrada:\n",
    "    Salida:\n",
    "    '''\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    countries = ['chile', 'japan', 'usa']\n",
    "    for country in countries:\n",
    "        \n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/earthquakes?country={country.upper()}&maxYear=2022&minYear=2000'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Creamos la columna con la fecha del sismo\n",
    "        df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']])\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'depth', 'latitude':'lat', 'longitude':'lng', 'locationName':'place', 'country':'pais'}, inplace=True)\n",
    "\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        df['title'] = None\n",
    "        df['tsunami'] = df.apply(lambda x: 1 if pd.isnull(x.tsunamiEventId) != True else 0, axis=1)\n",
    "        if country=='japan':\n",
    "            country='japon' \n",
    "        df['idpais']  = paises(country)\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "        \n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','mag','place', 'time', 'url', 'tsunami', 'title', 'lng',\n",
    "            'lat', 'depth']]\n",
    "        df['peligro']=1\n",
    "        df['year']=df['time'].apply(lambda x: x.year)\n",
    "        df['month']=df['time'].apply(lambda x: x.month)\n",
    "        df['day']=df['time'].apply(lambda x: x.day)\n",
    "\n",
    "        #Info para terminar de cargar la tabla\n",
    "        # df=limpieza_general_tabla(df)\n",
    "        # #Paso la tabla a Potgres        \n",
    "        df.to_sql(name=country,con=cone, if_exists='append', index=False)\n",
    "        print(f'La carga se ha hecho con exito!{country}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carga se ha hecho con exito!chile\n",
      "La carga se ha hecho con exito!japon\n",
      "La carga se ha hecho con exito!usa\n"
     ]
    }
   ],
   "source": [
    "sismos_Noaa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Volcanes`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultima erupción acorde con el program global de vulcanología del Smithsonian Institution.\n",
    "* D1\tUtima erupcion en 1964 o despues.\n",
    "* D2\tUtima erupcion entre 1900-1963.\n",
    "* D3\tUtima erupcion entre 1800-1899.\n",
    "* D4\tUtima erupcion entre 1700-1799\n",
    "* D5\tUtima erupcion entre 1500-1699\n",
    "* D6\tUtima erupcion entre A.D. 1-1499\n",
    "* D7\tUtima erupcion entre B.C. (Holocene)\n",
    "* U\tNo fechado, posiblemente en Holoceno\n",
    "* Q\tErupcion cuaternaria\n",
    "* ?\tErupción en el Holoceno incierta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volcanes_Noaa():\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    countries = ['chile', 'japan', 'United%20States']\n",
    "    for country in countries:\n",
    "        \n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanolocs?country={country.upper()}'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'name':'nombre', 'morphology':'tipo', 'elevation':'elevacion', 'latitude':'lat', 'longitude':'lng', 'location':'place', 'country':'pais', 'timeErupt':'ultima_erupcion'}, inplace=True)\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        if country == 'United%20States':\n",
    "            df['pais']  = df.pais.apply(lambda x: paises('usa') if x.lower() == 'united states' else x)\n",
    "            country='usa'\n",
    "        if country =='japan':\n",
    "            country='japon'\n",
    "        df['idpais']  = df.pais.apply(lambda x: paises(country))\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "\n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idpais','nombre','tipo', 'elevacion', 'place', 'ultima_erupcion', 'lat', 'lng','url']]\n",
    "    \n",
    "        #display(df)        \n",
    "        df.to_sql(name='volcanes',con=cone, if_exists='append', index=False)\n",
    "        print('La carga se ha hecho con exito!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carga se ha hecho con exito!\n",
      "La carga se ha hecho con exito!\n",
      "La carga se ha hecho con exito!\n"
     ]
    }
   ],
   "source": [
    "volcanes_Noaa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a06e0cf41d9bd1fb106e7e83326df00b44cf746455b39376adea504742b82eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
