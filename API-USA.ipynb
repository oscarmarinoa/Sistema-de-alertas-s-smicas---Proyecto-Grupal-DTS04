{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "#Para rellenar vacíos\n",
    "import numpy as np\n",
    "import json\n",
    "#ETL:\n",
    "\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#para normalizar fechas\n",
    "import datetime\n",
    "\n",
    "#Conexión con postgresql:\n",
    "\n",
    "#Para crear tablas con claves primarias y foraneas\n",
    "import psycopg2\n",
    "#Para append los datos a ingestar en la tabla\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Carga de datos a postgres:\n",
    "\n",
    "#Para tener errores en try y except\n",
    "import traceback\n",
    "\n",
    "#Ver para japón: (borrar si no se usa)\n",
    "from urllib.parse import quote\n",
    "import io\n",
    "\n",
    "#limpiar la consola\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`ETL`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_index(df):\n",
    "    '''\n",
    "    Normalizamos índice\n",
    "    '''\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_general_tabla (df):\n",
    "    '''\n",
    "    Función limpieza de cadenas de string\n",
    "    Devuelve:  el df ingestado con normalizaciones\n",
    "    '''\n",
    "    #Vemos duplicados y existen los eliminamos\n",
    "    df.drop_duplicates(inplace = True) \n",
    "    #Acomodamos el indice\n",
    "    df=normalizar_index(df)\n",
    "  \n",
    "    #recorremos cada columna del dataset con un bucle\n",
    "    for c in df.columns:         \n",
    "        #Detectamos las columnas que son string \n",
    "        if df[c].dtype == 'object':\n",
    "            #ponemos todo en minúsculas\n",
    "            df[c]=df[c].str.lower() \n",
    "            df[c]=df[c].apply(lambda x:x.strip() if type(x)!=float else x)\n",
    "            #creamos una lista de valores a reemplazar por vacío\n",
    "            lista_simbolos=['!',',',';','-','.',' ?','? ','?',':']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)                  \n",
    "            #creamos una lista de valores a reemplazar por espacio\n",
    "            lista_simbolos=['_','  ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)                  \n",
    "        #sacamos los acentos\n",
    "        df[c]=df[c].apply(lambda x: normalize( 'NFC', re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", x), 0, re.I))\n",
    "                                        if type(x)== str and x!= 0 and x!= 'NaN'\n",
    "                                        else x)\n",
    "\n",
    "        if c== 'place':\n",
    " \n",
    "            lista_simbolos=[' of ',' sw ',' w ',' n ']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,' ')if type(x)!=float else x)\n",
    "            #reemplazamos los '' por 'sin dato'\n",
    "            df[c]=df[c].apply(lambda x: 'sin dato' if type(x)== str and x=='' else x)\n",
    "            #sacamos los que no tengan el pais que buscamos\n",
    "            df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')] \n",
    "            #los eliminamos de place\n",
    "            lista_simbolos=['japan','chile']\n",
    "            for elemento in lista_simbolos:\n",
    "                df[c]=df[c].apply(lambda x:x.replace(elemento ,'')if type(x)!=float else x)\n",
    "\n",
    "        #detectamos NaN\n",
    "        df[c]=df[c].apply(lambda x: None if type(x)== str and x=='' else x)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nINTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\\n\\nfrom datetime import datetime\\nimport pytz\\n\\nlocal = datetime.now()\\nprint(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n\\ntz_NY = pytz.timezone(\\'America/New_York\\') \\ndatetime_NY = datetime.now(tz_NY)\\nprint(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\\n\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "INTENTO DE HACER UN CALENDARIO PARA RECORRER LA API, NO FUNCIONA BORRAR SI QUEDÓ\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "local = datetime.now()\n",
    "print(\"Local:\", local.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "\n",
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_NY = datetime.now(tz_NY)\n",
    "print(\"NY:\", datetime_NY.strftime(\"%m/%d/%Y, %H:%M:%S\"))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos (url,pais):\n",
    "    '''\n",
    "    Función para pasar de los datos de url de USGS\n",
    "    Toma de parámetro una url y devuelve el df que utilizaremos\n",
    "\n",
    "    '''\n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    for i, fila in enumerate(resp['features']):\n",
    "        \n",
    "        a=resp['features'][i]\n",
    "        #agrego al diccionario magnitud, tiempo, si produjo tsunami y sig\n",
    "        lista_propierties=['mag','time','tsunami','sig']\n",
    "        for elemento in lista_propierties:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(np.nan) \n",
    "            else:\n",
    "                dict[elemento].append(float(a['properties'][elemento]))\n",
    "                \n",
    "        \n",
    "        #agrego lugar, url con información a ampliar y el título     \n",
    "        lista_propierties2=['place','url','title']\n",
    "        for elemento in lista_propierties2:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append('Sin dato')\n",
    "            else:\n",
    "                dict[elemento].append(a['properties'][elemento])\n",
    "\n",
    "        #Agrego al diccionario latitud, longitud y profundidad\n",
    "        list_geometry=['lng','lat','deepth']\n",
    "        for indice, elemento in enumerate(list_geometry):\n",
    "            if a['geometry']['coordinates'][indice] is None:\n",
    "                dict[elemento].append(np.nan)\n",
    "            else:\n",
    "                dict[elemento].append(float(a['geometry']['coordinates'][indice]))\n",
    "\n",
    "    #Devuelvo el diccionario hecho df\n",
    "    return (pd.DataFrame(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultimoDia(anio, mes):\n",
    "    '''\n",
    "    Función ultimo día del mes\n",
    "    Devuelve el último día del mes\n",
    "\n",
    "    anio: año a procesar\n",
    "    mes: mes del cual obtener el último día\n",
    "    '''\n",
    "    #Valor por defecto\n",
    "    ultimo_dia = 31\n",
    "    #Meses con 30 días\n",
    "    if mes in [4, 6, 9, 11]:\n",
    "        ultimo_dia = 30\n",
    "\n",
    "    # Vemos si el año es bisiesto\n",
    "    if mes == 2:\n",
    "        if (anio % 4) == 0:\n",
    "            if (anio % 100) == 0:\n",
    "                if (anio % 400) == 0:\n",
    "                    ultimo_dia = 29\n",
    "                else:\n",
    "                    ultimo_dia = 28\n",
    "            else:\n",
    "                ultimo_dia = 29\n",
    "        else:\n",
    "            ultimo_dia = 28\n",
    "\n",
    "    return ultimo_dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***`Carga (Load - Potgresql)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conexión a postgres mediante alquemy\n",
    "cone = create_engine('postgresql://sismosu:123@localhost:5432/sismosdb', pool_size=50, max_overflow=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REALIZAMOS TABLAS DE MANERA TABLA MANUAL: para poner primarykey, clave foranea\n",
    "'''\n",
    "\n",
    "tabla_eeuu_usgs = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_chile_usgs = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_japon_usgs = 'DROP TABLE IF EXISTS JAPON CASCADE; CREATE TABLE JAPON (id SERIAL PRIMARY KEY NOT NULL  ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "tabla_hechos = 'DROP TABLE IF EXISTS SISMOS CASCADE; CREATE TABLE SISMOS (id SERIAL PRIMARY KEY NOT NULL ,mag float8,pais text, place text,time timestamp,url text,tsunami float8, sig float8, title text,lng float8, lat float8,deepth float8);'\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host='localhost',\n",
    "    user='sismosu',\n",
    "    password='123',\n",
    "    database='sismosdb',\n",
    "    port='5432'\n",
    " )\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(tabla_eeuu_usgs)\n",
    "cur.execute(tabla_chile_usgs)\n",
    "cur.execute(tabla_japon_usgs)\n",
    "cur.execute(tabla_hechos)\n",
    "\n",
    "conn.commit()\n",
    "cur.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Japón`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Es este: Si queda lo pongo en una función para que sea llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022/11/22 00:00:00.000000\n",
      "fin\n",
      "20221110\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>sig</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>deepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.2</td>\n",
       "      <td>珠洲市</td>\n",
       "      <td>2022/11/22 21:34:05.870</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>E137.2</td>\n",
       "      <td>N37.5</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.6</td>\n",
       "      <td>伊那市</td>\n",
       "      <td>2022/11/22 16:35:12.166</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>長野県南部</td>\n",
       "      <td>E138.1</td>\n",
       "      <td>N35.9</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1</td>\n",
       "      <td>?i?F?s</td>\n",
       "      <td>2022/11/22 16:01:42.236</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>?I?i?§”\\“o’n?u</td>\n",
       "      <td>E137.3</td>\n",
       "      <td>N37.5</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>珠洲市</td>\n",
       "      <td>2022/11/22 16:01:42.092</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>E137.3</td>\n",
       "      <td>N37.5</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.8</td>\n",
       "      <td>鹿児島十島村</td>\n",
       "      <td>2022/11/22 12:07:55.812</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>トカラ列島近海</td>\n",
       "      <td>E129.3</td>\n",
       "      <td>N29.2</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3.6</td>\n",
       "      <td>城里町</td>\n",
       "      <td>2022/11/11 14:32:15.765</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>茨城県南部</td>\n",
       "      <td>E140.0</td>\n",
       "      <td>N36.2</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>3.6</td>\n",
       "      <td>None</td>\n",
       "      <td>2022/11/11 14:31:20.228</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>茨城県南部</td>\n",
       "      <td>E140.0</td>\n",
       "      <td>N36.2</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3.0</td>\n",
       "      <td>珠洲市</td>\n",
       "      <td>2022/11/10 12:30:16.187</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>石川県能登地方</td>\n",
       "      <td>E137.2</td>\n",
       "      <td>N37.5</td>\n",
       "      <td>10km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4.2</td>\n",
       "      <td>銚子市</td>\n",
       "      <td>2022/11/10 10:26:11.086</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>千葉県東方沖</td>\n",
       "      <td>E141.1</td>\n",
       "      <td>N35.5</td>\n",
       "      <td>40km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.4</td>\n",
       "      <td>御坊市</td>\n",
       "      <td>2022/11/10 09:53:23.566</td>\n",
       "      <td>https://api.p2pquake.net/v1/human-readable?lim...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>和歌山県南部</td>\n",
       "      <td>E135.5</td>\n",
       "      <td>N33.9</td>\n",
       "      <td>50km</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mag   place                     time  \\\n",
       "0   3.2     珠洲市  2022/11/22 21:34:05.870   \n",
       "1   2.6     伊那市  2022/11/22 16:35:12.166   \n",
       "2   3.1  ?i?F?s  2022/11/22 16:01:42.236   \n",
       "3   3.1     珠洲市  2022/11/22 16:01:42.092   \n",
       "6   2.8  鹿児島十島村  2022/11/22 12:07:55.812   \n",
       "..  ...     ...                      ...   \n",
       "92  3.6     城里町  2022/11/11 14:32:15.765   \n",
       "93  3.6    None  2022/11/11 14:31:20.228   \n",
       "96  3.0     珠洲市  2022/11/10 12:30:16.187   \n",
       "97  4.2     銚子市  2022/11/10 10:26:11.086   \n",
       "99  3.4     御坊市  2022/11/10 09:53:23.566   \n",
       "\n",
       "                                                  url tsunami  sig  \\\n",
       "0   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "1   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "2   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "3   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "6   https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "..                                                ...     ...  ...   \n",
       "92  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "93  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "96  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "97  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "99  https://api.p2pquake.net/v1/human-readable?lim...    None    0   \n",
       "\n",
       "             title     lng    lat deepth  \n",
       "0          石川県能登地方  E137.2  N37.5   10km  \n",
       "1            長野県南部  E138.1  N35.9   10km  \n",
       "2   ?I?i?§”\\“o’n?u  E137.3  N37.5   10km  \n",
       "3          石川県能登地方  E137.3  N37.5   10km  \n",
       "6          トカラ列島近海  E129.3  N29.2   10km  \n",
       "..             ...     ...    ...    ...  \n",
       "92           茨城県南部  E140.0  N36.2   50km  \n",
       "93           茨城県南部  E140.0  N36.2   50km  \n",
       "96         石川県能登地方  E137.2  N37.5   10km  \n",
       "97          千葉県東方沖  E141.1  N35.5   40km  \n",
       "99          和歌山県南部  E135.5  N33.9   50km  \n",
       "\n",
       "[69 rows x 10 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creo dicc vacio\n",
    "dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "'''\n",
    "límite\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "número de devoluciones (1-100, el valor predeterminado es 10)\n",
    "'''\n",
    "limit=100\n",
    "'''\n",
    "compensar\n",
    "entero ($ int32 )\n",
    "( consulta )\t\n",
    "Número de registros para omitir \n",
    "(0 o más, el valor predeterminado es 0). \n",
    "Por ejemplo, limit=100&offset=200 devuelve 100 registros desde el 201.\n",
    "'''\n",
    "offset=0\n",
    "'''\n",
    "desde_fecha\n",
    "cuerda\n",
    "( consulta )\t\n",
    "Fecha especificada o posterior (formato aaaaMMdd)\n",
    "'''\n",
    "\n",
    "#fecha_start=time.time()\n",
    "fecha_start=20180501\n",
    "#print(datetime.datetime.strptime(fecha_start, '%Y/%m/%d %H:%M:%S'))\n",
    "print(datetime.date.today().strftime('%Y/%m/%d %H:%M:%S.%f'))\n",
    "\n",
    "'''\n",
    "fecha_end\n",
    "límite de las iteraciones\n",
    "\n",
    "'''\n",
    "fecha_end= 20220406\n",
    "#fecha_end= str(20210801)\n",
    "#fecha_end=datetime.datetime.strptime(fecha_end, '%Y%m%d')\n",
    "\n",
    "    \n",
    "while offset<=2100 and fecha_start>=fecha_end :\n",
    "    urlP2p = \"https://api.p2pquake.net/v1/human-readable?limit=100&offset={offset}&since_date={fecha_start}\"\n",
    "    #urlP2p = \"https://api.p2pquake.net/v1/human-readable?limit=100&offset={offset}\"\n",
    "    responseP2p = requests.get(urlP2p)\n",
    "    jsonDataP2p = responseP2p.json()\n",
    "    for i in range(limit):\n",
    "        try:\n",
    "            try:a=jsonDataP2p[i]\n",
    "            except:pass\n",
    "            \n",
    "            try:dict['mag'].append(a['earthquake']['hypocenter']['magnitude'])\n",
    "            except:dict['mag'].append(None)\n",
    "\n",
    "            try:dict['place'].append(a['points'][0]['addr'])\n",
    "            except:dict['place'].append(None)\n",
    "            \n",
    "            #try:dict['time'].append(datetime.datetime.strftime(a['time'], '%Y/%m/%d %H:%M:%S.%f'))            \n",
    "            try:dict['time'].append(a['time'])\n",
    "            except:dict['time'].append(None)\n",
    "            \n",
    "            indice=offset+i\n",
    "            try:dict['url'].append(\"https://api.p2pquake.net/v1/human-readable?limit=1&offset={indice}&since_date={fecha_start}\")\n",
    "            except:dict['url'].append(None)\n",
    "            \n",
    "            try:dict['tsunami'].append(a['earthquake'][\"domesticTsunami\"])\n",
    "            except:dict['tsunami'].append(None)\n",
    "                        \n",
    "            try:dict['title'].append(a['earthquake']['hypocenter']['name'])\n",
    "            except:dict['title'].append(None)\n",
    "            \n",
    "            try:dict['lng'].append(a['earthquake']['hypocenter']['longitude'])\n",
    "            except:dict['lng'].append(None)\n",
    "            \n",
    "            try:dict['lat'].append(a['earthquake']['hypocenter']['latitude'])\n",
    "            except:dict['lat'].append(None)\n",
    "            \n",
    "            try:dict['deepth'].append(a['earthquake']['hypocenter']['depth'][:-2])\n",
    "            except:dict['deepth'].append(None)\n",
    "        except: \n",
    "            pass\n",
    "    df=pd.DataFrame(dict)\n",
    "    fecha_min=df.time[-1:].values[0]\n",
    "    #print(fecha_min)\n",
    "    fecha_min=datetime.datetime.strptime(fecha_min, '%Y/%m/%d %H:%M:%S.%f')\n",
    "    fecha_min=int(datetime.datetime.strftime(fecha_min, '%Y%m%d'))\n",
    "    fecha_start=fecha_min\n",
    "    offset+=100\n",
    "    print(offset, fecha_min)\n",
    "    \n",
    "print('fin')\n",
    "print(fecha_min)\n",
    "#borro duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "#filtro filas con vacíos\n",
    "filtros=[None,'']\n",
    "for i in filtros:\n",
    "    m1=df['mag'].values !=i\n",
    "    m2=df['title'].values !=i\n",
    "    m3=df['lng'].values !=i\n",
    "    m4=df['lat'].values !=i\n",
    "    m5=df['deepth'].values !=i\n",
    "    df=df[m1 & m2 & m3 & m4 & m5]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***`Chile`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(date):\n",
    "    fecha='/'+date\n",
    "    year=date[:4]\n",
    "    month='/'+date[4:6]\n",
    "    url=\"https://www.sismologia.cl/sismicidad/catalogo/\"+year+month+fecha+\".html\"\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(date):\n",
    "    url=get_url(date)\n",
    "    urlx = requests.get (url)\n",
    "    soup = BeautifulSoup(urlx.content,\"html.parser\")\n",
    "    rows = soup.find(\"table\", attrs={\"class\":\"sismologia detalle\"}).find_all(\"tr\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datos_chile(url,rows):\n",
    "    #creo dicc vacio\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    for index,i in enumerate(rows):\n",
    "        if index==0:\n",
    "            pass\n",
    "        else:\n",
    "            dict['mag'].append(rows[index].find_all(\"td\")[-1].get_text()[:-3])\n",
    "            dict['place'].append(rows[index].find_all(\"td\")[0].get_text()[19:])\n",
    "            dict['time'].append(rows[index].find_all(\"td\")[1].get_text())\n",
    "\n",
    "            dict['url'].append(url)\n",
    "            dict['tsunami'].append(None)\n",
    "            dict['title'].append(None)\n",
    "\n",
    "            dict['lng'].append(rows[index].find_all(\"td\")[2].get_text()[:7])\n",
    "            dict['lat'].append(rows[index].find_all(\"td\")[2].get_text()[-7:])\n",
    "            dict['deepth'].append(rows[index].find_all(\"td\")[3].get_text()[:-3])\n",
    "    for elemento in dict:\n",
    "        len(dict[elemento])\n",
    "    df_chile=pd.DataFrame(dict)\n",
    "    return df_chile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_chile(date):\n",
    "    url=get_url(date)\n",
    "    rows=get_rows(date)\n",
    "    df_chile=datos_chile(url,rows)\n",
    "    return df_chile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=str(20201103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>deepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.8</td>\n",
       "      <td>35 km al S de Lebu</td>\n",
       "      <td>2020-11-03 20:59:50</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-37.926</td>\n",
       "      <td>-73.698</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>489 km al O de Melinka</td>\n",
       "      <td>2020-11-03 19:47:34</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-44.234</td>\n",
       "      <td>-79.851</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.1</td>\n",
       "      <td>45 km al S de Putre</td>\n",
       "      <td>2020-11-03 19:26:41</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-18.601</td>\n",
       "      <td>-69.586</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>41 km al S de Pica</td>\n",
       "      <td>2020-11-03 18:23:05</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-20.834</td>\n",
       "      <td>-69.193</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.3</td>\n",
       "      <td>35 km al O de Mina Collahuasi</td>\n",
       "      <td>2020-11-03 17:51:15</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-20.713</td>\n",
       "      <td>-68.971</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.8</td>\n",
       "      <td>25 km al SO de Los Vilos</td>\n",
       "      <td>2020-11-03 16:26:48</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-32.074</td>\n",
       "      <td>-71.691</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.8</td>\n",
       "      <td>18 km al SO de Los Vilos</td>\n",
       "      <td>2020-11-03 14:56:29</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-32.021</td>\n",
       "      <td>-71.651</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.2</td>\n",
       "      <td>47 km al NO de Quillagua</td>\n",
       "      <td>2020-11-03 12:16:03</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-21.361</td>\n",
       "      <td>-69.854</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.5</td>\n",
       "      <td>78 km al E de Socaire</td>\n",
       "      <td>2020-11-03 10:51:31</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-23.850</td>\n",
       "      <td>-67.180</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.5</td>\n",
       "      <td>51 km al SO de Ollagüe</td>\n",
       "      <td>2020-11-03 09:35:26</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-21.609</td>\n",
       "      <td>-68.520</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.7</td>\n",
       "      <td>13 km al O de Tongoy</td>\n",
       "      <td>2020-11-03 08:29:02</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-30.263</td>\n",
       "      <td>-71.629</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>47 km al SE de Pica</td>\n",
       "      <td>2020-11-03 02:45:04</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-20.875</td>\n",
       "      <td>-69.133</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.0</td>\n",
       "      <td>436 km al O de Melinka</td>\n",
       "      <td>2020-11-03 02:40:56</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-44.178</td>\n",
       "      <td>-79.192</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.8</td>\n",
       "      <td>54 km al NE de Chañaral.</td>\n",
       "      <td>2020-11-03 02:22:33</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-25.929</td>\n",
       "      <td>-70.348</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>26 km al NE de Calama</td>\n",
       "      <td>2020-11-03 01:08:44</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-22.262</td>\n",
       "      <td>-68.821</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.6</td>\n",
       "      <td>22 km al O de Mina Collahuasi</td>\n",
       "      <td>2020-11-03 00:57:27</td>\n",
       "      <td>https://www.sismologia.cl/sismicidad/catalogo/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-20.874</td>\n",
       "      <td>-68.860</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mag                          place                 time  \\\n",
       "0    3.8             35 km al S de Lebu  2020-11-03 20:59:50   \n",
       "1   4.9          489 km al O de Melinka  2020-11-03 19:47:34   \n",
       "2    4.1            45 km al S de Putre  2020-11-03 19:26:41   \n",
       "3    3.1             41 km al S de Pica  2020-11-03 18:23:05   \n",
       "4    3.3  35 km al O de Mina Collahuasi  2020-11-03 17:51:15   \n",
       "5    2.8       25 km al SO de Los Vilos  2020-11-03 16:26:48   \n",
       "6    3.8       18 km al SO de Los Vilos  2020-11-03 14:56:29   \n",
       "7    3.2       47 km al NO de Quillagua  2020-11-03 12:16:03   \n",
       "8    3.5          78 km al E de Socaire  2020-11-03 10:51:31   \n",
       "9    4.5         51 km al SO de Ollagüe  2020-11-03 09:35:26   \n",
       "10   2.7           13 km al O de Tongoy  2020-11-03 08:29:02   \n",
       "11   3.0            47 km al SE de Pica  2020-11-03 02:45:04   \n",
       "12  6.0          436 km al O de Melinka  2020-11-03 02:40:56   \n",
       "13   2.8       54 km al NE de Chañaral.  2020-11-03 02:22:33   \n",
       "14   3.0          26 km al NE de Calama  2020-11-03 01:08:44   \n",
       "15   2.6  22 km al O de Mina Collahuasi  2020-11-03 00:57:27   \n",
       "\n",
       "                                                  url tsunami title      lng  \\\n",
       "0   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -37.926   \n",
       "1   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -44.234   \n",
       "2   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -18.601   \n",
       "3   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -20.834   \n",
       "4   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -20.713   \n",
       "5   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -32.074   \n",
       "6   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -32.021   \n",
       "7   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -21.361   \n",
       "8   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -23.850   \n",
       "9   https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -21.609   \n",
       "10  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -30.263   \n",
       "11  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -20.875   \n",
       "12  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -44.178   \n",
       "13  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -25.929   \n",
       "14  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -22.262   \n",
       "15  https://www.sismologia.cl/sismicidad/catalogo/...    None  None  -20.874   \n",
       "\n",
       "        lat deepth  \n",
       "0   -73.698     34  \n",
       "1   -79.851     10  \n",
       "2   -69.586    117  \n",
       "3   -69.193    111  \n",
       "4   -68.971    108  \n",
       "5   -71.691     24  \n",
       "6   -71.651     35  \n",
       "7   -69.854     50  \n",
       "8   -67.180    200  \n",
       "9   -68.520    126  \n",
       "10  -71.629     36  \n",
       "11  -69.133    106  \n",
       "12  -79.192     10  \n",
       "13  -70.348     66  \n",
       "14  -68.821    115  \n",
       "15  -68.860     94  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chile=carga_chile(date)\n",
    "df_chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chile.to_csv('chile_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acá: Sacar última información de japón, para alimentar db y crear alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-20T18:21:00+09:00\n",
      "2022-11-20 18:21:00\n",
      "https://www.jma.go.jp/bosai/quake/data/20221120182125_20221120181734_VXSE5k_1.json\n",
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mag</th>\n",
       "      <th>place</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>tsunami</th>\n",
       "      <th>sig</th>\n",
       "      <th>title</th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "      <th>int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.7</td>\n",
       "      <td>Off the Coast of Kushiro</td>\n",
       "      <td>2022-11-20T18:21:00+09:00</td>\n",
       "      <td>https://www.jma.go.jp/bosai/quake/data/2022112...</td>\n",
       "      <td>This earthquake poses no tsunami risk.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>震源・震度情報</td>\n",
       "      <td>144.38</td>\n",
       "      <td>42.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mag                     place                       time  \\\n",
       "0  4.7  Off the Coast of Kushiro  2022-11-20T18:21:00+09:00   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.jma.go.jp/bosai/quake/data/2022112...   \n",
       "\n",
       "                                    tsunami  sig    title     lng    lat int  \n",
       "0  This earthquake poses no tsunami risk.\\n    0  震源・震度情報  144.38  42.99   3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'df=pd.DataFrame(dict)\\ndf.head()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creo dicc vacio\n",
    "dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'sig':[],'title':[],'lng':[],'lat':[],'int':[]}\n",
    "#pido los últimos sismos\n",
    "urlJMAList = \"https://www.jma.go.jp/bosai/quake/data/list.json\"\n",
    "#hago una lista de los últimos sismos\n",
    "responseJMAList = requests.get(urlJMAList)\n",
    "#la convierto a formato json\n",
    "jsonDataJMAList = responseJMAList.json()\n",
    "\n",
    "i=0\n",
    "#recorro la lista y solicito la info a la api\n",
    "#for elemento in range(len(jsonDataJMAList)):\n",
    "jsonDataJMALastDatetime = jsonDataJMAList[0][\"rdt\"]\n",
    "print(jsonDataJMALastDatetime)\n",
    "jsonDataJMALastDatetime = datetime.datetime.strptime(jsonDataJMALastDatetime, '%Y-%m-%dT%H:%M:%S+09:00')\n",
    "print(jsonDataJMALastDatetime)\n",
    "urlJMAEarthquakeData = \"https://www.jma.go.jp/bosai/quake/data/\" + jsonDataJMAList[i][\"json\"]\n",
    "print(urlJMAEarthquakeData)\n",
    "responseJMAEarthquakeData = requests.get(urlJMAEarthquakeData)\n",
    "print(responseJMAEarthquakeData)\n",
    "jsonDataJMAEarthquakeData = responseJMAEarthquakeData.json()\n",
    "#'mag'\n",
    "dict['mag'].append(jsonDataJMAEarthquakeData['Body']['Earthquake']['Magnitude'])\n",
    "#'place\n",
    "dict['place'].append(jsonDataJMAEarthquakeData['Body']['Earthquake']['Hypocenter']['Area']['enName'] )\n",
    "#time\n",
    "dict['time'].append(jsonDataJMAEarthquakeData['Head']['ReportDateTime'])\n",
    "#url\n",
    "dict['url'].append(urlJMAEarthquakeData)\n",
    "#tsunami\n",
    "dict['tsunami'].append(jsonDataJMAEarthquakeData['Body']['Comments']['ForecastComment']['enText'])\n",
    "#sig\n",
    "dict['sig'].append(0)\n",
    "#title\n",
    "dict['title'].append(jsonDataJMAEarthquakeData['Head']['Title'])\n",
    "\n",
    "#lat, long\n",
    "dict['lat'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lat'])\n",
    "\n",
    "dict['lng'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['City'][0]['IntensityStation'][0]['latlon']['lon'])\n",
    "\n",
    "#intensidad:\n",
    "dict['int'].append(jsonDataJMAEarthquakeData['Body'][\"Intensity\"][\"Observation\"][\"Pref\"][0]['Area'][0]['MaxInt'])\n",
    "i+=1\n",
    "df=pd.DataFrame(dict)\n",
    "display(df.head())   \n",
    "'''df=pd.DataFrame(dict)\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlJMAEarthquakeData = \"https://api.p2pquake.net/v2/jma/quake\" + jsonDataJMAList[0][\"json\"]\n",
    "responseJMAEarthquakeData = requests.get(urlJMAEarthquakeData)\n",
    "print(responseJMAEarthquakeData)\n",
    "jsonDataJMAEarthquakeData = responseJMAEarthquakeData.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jma.go.jp/bosai/quake/data/20221119095803_20221119095504_VXSE5k_1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***`Carga de datos de USGS`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def cargar_paises(anio,mes,dia_fin):\n",
    "    \n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en tres tablas distintas\n",
    "    Sirve para respaldo\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['usa', 'japon', 'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "    error=[]\n",
    "    for pais in paises:\n",
    "        while anio > 1999:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name=pais,con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta)\n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "                error.append(traceback.print_exc())\n",
    "            # Decrementamos el mes\n",
    "            if mes == 1:\n",
    "                # Reducimos el año\n",
    "                anio -= 1\n",
    "                mes = 12\n",
    "            else:\n",
    "                mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "    errores['error']=error\n",
    "    display(errores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "#Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises(anio,mes,dia_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df=limpieza_general_tabla(df)\n",
    "df=df[df.place.str.contains('japan|chile')|df.pais.str.contains('usa')]\n",
    "\n",
    "ERROR: raise AttributeError(\"Can only use .str accessor with string values!\")\n",
    "\n",
    "Error en la carga en la fecha: 2020-6-01 hasta 2020-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2019-7-01 hasta 2019-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-7-01 hasta 2018-7-31 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2018-6-01 hasta 2018-6-30 del país: usa\n",
    "\n",
    "Error en la carga en la fecha: 2010-4-01 hasta 2010-4-30 del país: usa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\\ndf['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\\ndf['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\\ndf['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\\ndf['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\\ndf['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df = pd.DataFrame([key for key in clients.keys()], columns=['Name'])\n",
    "df['id'] = [value['id'] if 'id' in value.keys() else None for value in clients.values()]\n",
    "df['email'] = [value['email'] if 'email' in value.keys() else None for value in clients.values()]\n",
    "df['gender'] = [value['gender'] if 'gender' in value.keys() else None for value in clients.values()]\n",
    "df['ip_address'] = [value['ip_address'] if 'ip_address' in value.keys() else None for value in clients.values()]\n",
    "df['money'] = [value['money'] if 'money' in value.keys() else None for value in clients.values()]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_paises_sismos(anio,mes,dia_fin):\n",
    "    '''\n",
    "    Carga en postgres para cada mes los sismos de los tres paises (japón, usa ,chile) para el año y mes indicado en una única tabla de hechos\n",
    "    Sirve para automatizar la carga incremental\n",
    "    \n",
    "    '''\n",
    "    #Lista de paises\n",
    "    paises = ['japon', 'usa' ,'chile']\n",
    "    fecha_inicio_error=[]\n",
    "    fecha_final_error=[]\n",
    "    pais_error=[]\n",
    "\n",
    "    while anio > 2000:\n",
    "        #recorre la lista de paises\n",
    "        for pais in paises:\n",
    "            ultimo_dia=ultimoDia(anio, mes)\n",
    "            # Armamos la cadena de fechas\n",
    "            fecha_desde = f'{str(anio)}-{str(mes)}-01'\n",
    "            fecha_hasta = f'{str(anio)}-{str(mes)}-{str(ultimo_dia)}'\n",
    "            try:\n",
    "                # Hacemos la consulta a la API en función del país de interés\n",
    "                if pais == 'usa':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&jsonerror=true'\n",
    "                elif pais == 'japon':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&jsonerror=true'\n",
    "                elif pais == 'chile':\n",
    "                    url = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={fecha_desde}&endtime={fecha_hasta}&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-68.900000&jsonerror=true'\n",
    "\n",
    "                #paso a df\n",
    "                df = guardar_datos (url,pais)\n",
    "                #corregimos las fechas\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Le decimos que es formato fecha\n",
    "                df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "                #Agrego columna con el nombre de país:\n",
    "                df['pais']=pais\n",
    "\n",
    "                #normalizamos cadenas de string\n",
    "                df=limpieza_general_tabla(df)\n",
    "\n",
    "                #Paso la tabla a Potgres\n",
    "                df.to_sql(name='sismos',con=cone, if_exists='append', index=False)\n",
    "                print('La carga se ha hecho con exito!en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "            \n",
    "            except:\n",
    "                #imprimo los errores\n",
    "                #traceback.print_exc()\n",
    "                print('Error en la carga en la fecha:', fecha_desde,'hasta',fecha_hasta,'del país:',pais)\n",
    "                fecha_inicio_error.append(fecha_desde)\n",
    "                fecha_final_error.append(fecha_hasta)\n",
    "                pais_error.append(pais)\n",
    "\n",
    "\n",
    "        # Decrementamos el mes\n",
    "        if mes == 1:\n",
    "            # Reducimos el año\n",
    "            anio -= 1\n",
    "            mes = 12\n",
    "        else:\n",
    "            mes -= 1 \n",
    "    errores=pd.DataFrame()\n",
    "    errores['fecha_inicio']=fecha_inicio_error\n",
    "    errores['fecha_final']=fecha_final_error\n",
    "    errores['pais']=pais_error\n",
    "\n",
    "    display(errores)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutamos la función\n",
    "# Configuramos la fecha de inicio\n",
    "anio = 2022\n",
    "mes = 11\n",
    "dia_fin = 0\n",
    "cargar_paises_sismos(anio,mes,dia_fin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2852b27bee67d373d5d537f6ee9474570f8f7363f24457c4522e6a7b2aeb81c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
